
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from timm.scheduler import CosineLRScheduler
from pydantic import Field

from endaaman.ml import BaseMLCLI, BaseDLArgs, BaseTrainer, BaseTrainerConfig
from endaaman.metrics import BaseMetrics, AccuracyByChannel



class CNNModel(nn.Module):
    def __init__(self, input_size=1, output_size=10):
        super().__init__()
        self.output_size = output_size
        self.convs = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, 3, 1, 1),
            nn.ReLU(inplace=True),
        )
        self.pool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Linear(16 * 16, output_size)

    def forward(self, x, activate=False):
        x = self.convs(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        # mean over batch
        x = torch.mean(x, dim=0)
        x = self.fc(x)

        if activate:
            if self.output_size == 1:
                x = F.sigmoid(x)
            else:
                x = F.log_softmax(x, dim=1)
        return x

class TrainerConfig(BaseTrainerConfig):
    grouped: bool = False


class Acc(BaseMetrics):
    def calc(self, preds, gts):
        correct = torch.sum(torch.argmax(preds, dim=-1) == gts)
        return correct / len(preds)

class Trainer(BaseTrainer):
    def prepare(self):
        model = CNNModel(input_size=1, output_size=1)
        self.criterion = nn.BCELoss()
        # self.criterion = nn.CrossEntropyLoss()
        return model

    def eval(self, inputs, gts):
        preds = self.model(inputs.to(self.device), activate=True)
        gts = torch.sum(gts == 0).float()[None]
        print(preds)
        loss = self.criterion(preds, gts.to(self.device))
        return loss, preds

    def get_metrics(self):
        return {
            'acc': Acc(),
        }

class CLI(BaseMLCLI):
    class TrainArgs(BaseDLArgs):
        name:str = 'mnist2'
        batch_size:int = 9
        overwrite: bool = Field(False, cli=('--overwrite', ))

    def run_train(self, a:TrainArgs):
        transform = transforms.Compose([
            transforms.ToTensor(),
        ])
        dss = [datasets.MNIST(
            root='./datasets/MNIST',
            train=t,
            download=True,
            transform=transform
        ) for t in [True, False] ]

        dss[0].data = dss[0].data[:10000, ...]
        dss[1].data = dss[1].data[:1000, ...]

        config = TrainerConfig(
            lr=0.001,
            batch_size=a.batch_size,
            num_workers=a.num_workers,
        )

        t = Trainer(
            config=config,
            out_dir=f'out/models/{a.name}',
            train_dataset=dss[0],
            val_dataset=dss[1],
            use_gpu=not a.cpu,
            overwrite=a.overwrite,
        )
        t.start(a.epoch)

if __name__ == '__main__':
    cli = CLI()
    cli.run()
