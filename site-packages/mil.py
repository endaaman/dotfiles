import os
import time

from tqdm import tqdm
import click

import pandas as pd
import numpy as np
import matplotlib
from matplotlib import pyplot as plt
import torch
from torch import optim, nn
import torch.nn.functional as F
from torch.utils.data import Dataset, Subset, DataLoader
from torchvision.utils import make_grid
from torchvision import datasets, transforms
from torchvision.models.resnet import ResNet, BasicBlock
from tensorboardX import SummaryWriter

from endaaman.ml import BaseDLCLI, BaseTrainer, BaseTrainerConfig


TILE_COUNT = 9


class MILDataset(Dataset):
    def __init__(self, train=True):
        self.train = train
        self.base = datasets.MNIST(
            './datasets/MNIST',
            train = train,
            download = True,
            transform = transforms.Compose([
                transforms.ToTensor()
            ]),
        )
        self.data = self.base.data
        self.labels = self.base.targets

        idx = np.arange(len(self.data))
        np.random.shuffle(idx)
        self.idx = idx
        self.total = len(idx) // TILE_COUNT

    def __len__(self):
        return self.total

    def __getitem__(self, i):
        target_idx = self.idx[i*TILE_COUNT : i*TILE_COUNT+TILE_COUNT]
        xx = self.data[target_idx]
        yy = self.labels[target_idx]

        y = torch.any(yy == 0)[None].float()
        xx = xx / 255
        xx = xx.unsqueeze(dim=1)
        return xx, y


class MILModel(nn.Module):
    def __init__(self, num_classes, in_channels=1, params_count=100):
        super().__init__()
        self.num_classes = num_classes

        c = 16
        p = 4
        self.convs = nn.Sequential(
            nn.Conv2d(in_channels, 32, 3, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, c, 3, 1),
            nn.ReLU(inplace=True),
        )
        self.pool = nn.AdaptiveAvgPool2d(p)
        num_features = c * p * p

        self.u = nn.Parameter(torch.randn(params_count, num_features))
        self.v = nn.Parameter(torch.randn(params_count, num_features))
        self.w = nn.Parameter(torch.randn(params_count, 1))

        self.fc = nn.Linear(num_features, num_classes)


    def compute_attention_scores(self, x):
        '''
        Args:
            x (Tensor): (C, ) logits by instance
        Returns:
            Tensor: (1, )
        '''
        # x: (i, ) u: (p, i, ) v: (p, i, )
        xu = torch.tanh(torch.matmul(self.u, x))
        xv = torch.sigmoid(torch.matmul(self.v, x))
        # xu: (p, ) xu: (p, )
        x = xu * xv
        # x: (p, ) w: (p, i, )
        alpha = torch.matmul(x, self.w)
        return alpha

    def forward_attentions(self, features):
        '''
        Args:
            features (Tensor): (B, C) batched features
        Returns:
            Tensor: attention P-values (B, ) [0, 1]
        '''
        attentions = []
        for pred in features:
            attentions.append(self.compute_attention_scores(pred))
        attentions = torch.stack(attentions)
        attentions = torch.softmax(attentions.flatten(), dim=0)
        return attentions

    def forward_features(self, x):
        # x = self.base(x)
        # return torch.sigmoid(x)
        x = self.convs(x)
        x = self.pool(x)
        features = torch.flatten(x, 1)
        return features

    def forward(self, x, activate=False):
        features = self.forward_features(x)
        # attentions = self.forward_attentions(features)
        # features = torch.matmul(attentions, features)

        features = torch.mean(features, dim=-2)

        x = self.fc(features)
        if activate:
            if self.num_classes > 1:
                x = torch.softmax(x, dim=-1)
            else:
                x = torch.sigmoid(x)
        return x

class TrainerConfig(BaseTrainerConfig):
    batch_size:int = 1
    num_workers:int = 1
    lr: float = 0.001


class Trainer(BaseTrainer):
    def prepare(self):
        model = MILModel(num_classes=1)
        self.criterion = nn.BCELoss()
        return model

    def eval(self, inputs, gts):
        self.model = self.model.to(self.device)
        inputs = inputs[0]
        gts = gts[0]
        preds = self.model(inputs.to(self.device))
        loss = self.criterion(preds, gts.to(self.device))
        return loss, preds


class CLI(BaseDLCLI):
    def run_model(self, a):
        m = MILModel(num_classes=1)
        t = torch.randn(5, 1, 28, 28)
        print(m(t))

    def run_t(self, a):
        config = TrainerConfig()
        trainer = Trainer(
            config=config,
            out_dir='out/models',
            train_dataset=MILDataset(train=True),
            val_dataset=MILDataset(train=False),
            use_gpu=True,
        )
        trainer.start(50)


    def run_train(self, a):
        EPOCH = 100
        THRESHOLD = 0.5
        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

        train_dataset = MILDataset(train=True)
        val_dataset = MILDataset(train=False)
        model = MILModel(num_classes=1).to(device)
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.BCELoss()

        print('start training')

        base_logdir = 'logs/mil'
        logdir = base_logdir
        i = 0
        while os.path.exists(logdir):
            logdir = base_logdir + f'_{i}'
            i += 1
        writer = SummaryWriter(logdir=logdir)
        print(f'using {logdir}')
        for epoch in range(EPOCH):
            print(f'[{epoch}/{EPOCH}]')
            # l = len(train_dataset)
            l = 2000
            t_batch = tqdm(range(l), leave=False)

            losses = []
            corrects = 0
            for i in t_batch:
                xx, gt = train_dataset[i]

                optimizer.zero_grad()
                xx = xx.to(device)
                gt = gt.to(device)

                pred = model(xx, activate=True)
                corrects += torch.logical_and(pred > THRESHOLD, gt > THRESHOLD).squeeze().item()

                loss = criterion(pred, gt)
                loss.backward()
                losses.append(loss.item())
                optimizer.step()
                t_batch.set_description(f'train: loss:{loss:.5f}')
                t_batch.refresh()

            loss = np.mean(losses)
            acc = corrects / l
            print(f'train loss: {loss:.5f} acc:{acc:.2f}')
            writer.add_scalar('loss/train', loss, epoch)
            writer.add_scalar('acc/train', acc, epoch)
            writer.flush()

            l = 500
            t_batch = tqdm(range(l), leave=False)
            losses = []
            corrects = 0
            for i in t_batch:
                xx, gt = val_dataset[i]
                xx = xx.to(device)
                gt = gt.to(device)
                with torch.no_grad():
                    pred = model(xx, activate=True)
                loss = criterion(pred, gt)
                corrects += torch.logical_and(pred > THRESHOLD, gt > THRESHOLD).squeeze().item()
                losses.append(loss.item())
                t_batch.set_description(f'test: loss:{loss:.5f}')
                t_batch.refresh()

            loss = np.mean(losses)
            acc = corrects / l
            print(f'val loss: {loss:.5f} acc:{acc:.2f}')
            writer.add_scalar('loss/val', loss, epoch)
            writer.add_scalar('acc/val', acc, epoch)
            writer.flush()

if __name__ == '__main__':
    cli = CLI()
    cli.run()
