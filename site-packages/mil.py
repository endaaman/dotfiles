import os
import cv2
from matplotlib import pyplot as plt
from pydantic import Field
from sklearn import metrics
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from timm.scheduler import CosineLRScheduler

from endaaman.ml import BaseMLCLI, BaseDLArgs, BaseTrainer, BaseTrainerConfig, Checkpoint
from endaaman.metrics import BaseMetrics, AccuracyByChannel



class CNNModel(nn.Module):
    def __init__(self, mode, output_size=1, params_count=10):
        super().__init__()
        self.mode = mode
        self.output_size = output_size
        self.convs = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, 3, 1, 1),
            nn.ReLU(inplace=True),
        )
        self.pool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Linear(16 * 16, output_size)

        if mode == 'attention':
            self.u = nn.Parameter(torch.randn(params_count, 16*4*4))
            self.v = nn.Parameter(torch.randn(params_count, 16*4*4))
            self.w = nn.Parameter(torch.randn(params_count, 1))


    def compute_attention_scores(self, x):
        xu = torch.tanh(torch.matmul(self.u, x))
        xv = torch.sigmoid(torch.matmul(self.v, x))
        x = xu * xv
        alpha = torch.matmul(x, self.w)
        return alpha

    def compute_attentions(self, features):
        aa = []
        for feature in features:
            aa.append(self.compute_attention_scores(feature))
        aa = torch.stack(aa).flatten()
        aa = torch.softmax(aa, dim=0)
        return aa

    def forward(self, x, activate=True, with_attentions=False):
        x = self.convs(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        if self.mode == 'mean':
            x = torch.mean(x, dim=0)
        else:
            aa = self.compute_attentions(x)
            x = x * aa[:, None]
            x = x.sum(dim=0)
        x = self.fc(x)
        if activate:
            x = torch.sigmoid(x)

        if with_attentions:
            return x, aa.detach()
        return x


class AccMetrics(BaseMetrics):
    def calc(self, preds, gts):
        gts = gts.reshape(preds.shape[0], -1)
        preds = preds.flatten() > 0.5
        gts = torch.any(gts == 0, dim=1)[None]
        return torch.sum(preds == gts) / len(preds)

class ROCMetrics(BaseMetrics):
    def calc(self, preds, gts):
        if len(preds) < 2:
            return None
        gts = gts.reshape(preds.shape[0], -1)
        gts = torch.any(gts == 0, dim=1)
        fpr, tpr, __thresholds = metrics.roc_curve(gts.detach().numpy(), preds.detach().numpy())
        auc = metrics.auc(fpr, tpr)
        youden_index = np.argmax(tpr - fpr)
        return auc, tpr[youden_index], -fpr[youden_index]+1


class TrainerConfig(BaseTrainerConfig):
    mode:str = 'mean'

class Trainer(BaseTrainer):
    def prepare(self):
        model = CNNModel(mode=self.config.mode)
        self.criterion = nn.BCELoss()
        # self.criterion = nn.CrossEntropyLoss()
        return model

    def eval(self, inputs, gts):
        preds = self.model(inputs.to(self.device), activate=True)
        gts = torch.any(gts == 0).float()[None]
        loss = self.criterion(preds, gts.to(self.device))
        return loss, preds

    def get_metrics(self):
        return {
            'acc': AccMetrics(),
            'auc|recall|spec': ROCMetrics(),
        }

class CLI(BaseMLCLI):
    class TrainArgs(BaseDLArgs):
        mode:str = 'mean'
        epoch:int = 20
        batch_size:int = 8
        overwrite: bool = Field(False, cli=('--overwrite', ))

    def run_train(self, a:TrainArgs):
        dss = [datasets.MNIST(
            root='./datasets/MNIST',
            train=t,
            download=True,
            transform=transforms.ToTensor(),
        ) for t in [True, False] ]

        dss[0].data = dss[0].data[:10000, ...]
        dss[1].data = dss[1].data[:1000, ...]

        config = TrainerConfig(
            mode=a.mode,
            batch_size=a.batch_size,
            lr=0.001,
            num_workers=a.num_workers,
        )

        t = Trainer(
            config=config,
            out_dir=f'out/models/{a.mode}_{a.batch_size}',
            train_dataset=dss[0],
            val_dataset=dss[1],
            use_gpu=not a.cpu,
            overwrite=a.overwrite,
        )
        t.start(a.epoch)

    class ModelArgs(BaseMLCLI.CommonArgs):
        mode:str = 'attention'

    def run_model(self, a):
        model = CNNModel(mode=a.mode, output_size=1)
        x = torch.randn(7, 1, 28, 28)
        y = model(x)
        print(y.shape)


    class PredArgs(BaseMLCLI.CommonArgs):
        checkpoint:str = Field(..., cli=('--checkpoint', '-C'))
        count:int = 10

    def run_pred(self, a:PredArgs):
        c:Checkpoint = torch.load(self.a.checkpoint)
        ds = datasets.MNIST(
            root='./datasets/MNIST',
            train=False,
            download=True,
            transform=transforms.ToTensor(),
        )

        model = CNNModel(mode=c.config['mode'], output_size=1)
        model.load_state_dict(c.model_state)

        idx = np.random.choice(ds.data.shape[0], a.count)
        x = ds.data[idx][:, None, :, :] / 255
        # gt = ds.targets[idx]

        y, aa = model(x, with_attentions=True)
        print(y)

        plt.subplot(2,1,1)
        plt.title(f'pred: {y.item():.3f}')
        plt.imshow(cv2.hconcat(x.numpy().squeeze()))
        plt.subplot(2,1,2)
        plt.pcolor([aa.numpy()])
        d = os.path.dirname(a.checkpoint)
        plt.savefig(os.path.join(d, f'{a.count}_{a.seed}.png'))
        plt.show()


if __name__ == '__main__':
    cli = CLI()
    cli.run()
