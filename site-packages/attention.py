import os
import time

from pydantic import Field
from tqdm import tqdm
import pandas as pd
import numpy as np
import matplotlib
from matplotlib import pyplot as plt
import torch
from torch import optim, nn
import torch.nn.functional as F
from torch.utils.data import Dataset, Subset, DataLoader
from torchvision.utils import make_grid
from torchvision import datasets, transforms
from torchvision.models.resnet import ResNet, BasicBlock
from tensorboardX import SummaryWriter

from endaaman.ml import BaseMLCLI, BaseTrainer, BaseTrainerConfig


class AttentionDataset(Dataset):
    def __init__(self, total, count, loc_0, loc_1, scale):
        self.total = total
        self.count = count
        self.data = np.stack([
            np.random.normal(loc=loc_0, scale=scale, size=total*count//2),
            np.random.normal(loc=loc_1, scale=scale, size=total*count//2),
        ])

    def __len__(self):
        return self.total

    def __getitem__(self, idx):
        y = idx % 2
        i = idx // 2

        x = self.data[y, i*self.count : (i+1)*self.count]
        x = torch.from_numpy(x.astype(np.float32))
        y = torch.tensor([y]).float()
        return x, y


class AttentionModel(nn.Module):
    def __init__(self, num_features=3, num_classes=1, params_count=10):
        super().__init__()
        self.u = nn.Parameter(torch.randn(params_count, num_features))
        self.v = nn.Parameter(torch.randn(params_count, num_features))
        self.w = nn.Parameter(torch.randn(params_count, 1))

        self.fc = nn.Linear(num_features, num_classes)

    def compute_attention_scores(self, x):
        '''
        Args:
            x (Tensor): (C, ) logits by instance
        Returns:
            Tensor: (1, )
        '''
        # x: (i, ) u: (p, i, ) v: (p, i, )
        xu = torch.tanh(torch.matmul(self.u, x))
        xv = torch.sigmoid(torch.matmul(self.v, x))
        # xu: (p, ) xu: (p, )
        x = xu * xv
        # x: (p, ) w: (p, i, )
        alpha = torch.matmul(x, self.w)
        return alpha

    def compute_attentions(self, features):
        '''
        Args:
            features (Tensor): (B, C) batched features
        Returns:
            Tensor: attention P-values (B, ) [0, 1]
        '''
        aa = []
        for feature in features:
            aa.append(self.compute_attention_scores(feature))

        aa = torch.stack(aa).flatten()
        # aa = aa - aa.min()
        # aa = aa / aa.sum()
        aa = torch.softmax(aa, dim=0)
        return aa

    def forward(self, x, activate=False):
        # aa = self.compute_attentions(x)
        # x = x * aa[:, None]
        x = x.mean(dim=0)[None, ...]
        x = self.fc(x)
        x = torch.sum(x, dim=0)
        if activate:
            x = torch.sigmoid(x)
        return x


class LinearModel(nn.Module):
    def __init__(self, num_features=3, num_classes=1):
        super().__init__()
        self.fc = nn.Linear(num_features, num_classes)

    def forward(self, x, activate=False):
        x = self.fc(x)
        if activate:
            x = torch.sigmoid(x)
        return x

class CommonTrainerConfig(BaseTrainerConfig):
    lr: float = 0.01
    num_workers:int = 1

    total:int
    count:int
    loc_0:int
    loc_1:int
    scale:int


class AttentionTrainerConfig(CommonTrainerConfig):
    batch_size:int = 5
    num_features: int = 3

    # model
    params_count: int = 10


class AttentionTrainer(BaseTrainer):
    def prepare(self):
        model = AttentionModel(num_features=self.config.num_features, params_count=self.config.params_count)
        self.criterion = nn.BCELoss()
        return model

    def eval(self, inputs, gts):
        self.model.to(self.device)
        preds = self.model(inputs.to(self.device), activate=True)
        gts = (torch.mean(gts) > 0.5).float()
        gts = gts.to(self.device)[None]
        loss = self.criterion(preds, gts)
        return loss, None



class LinearTrainerConfig(CommonTrainerConfig):
    batch_size:int = 5
    num_features: int

class LinearTrainer(BaseTrainer):
    def prepare(self):
        model = LinearModel(num_features=self.config.num_features, num_classes=1)
        self.criterion = nn.BCELoss()
        return model

    def eval(self, inputs, gts):
        self.model.to(self.device)
        preds = self.model(inputs.to(self.device), activate=True)
        # make weak-label
        # preds = preds.mean(dim=0)
        # gts = (gts.mean(dim=0) > 0.5).float()

        gts = gts.to(self.device)

        loss = self.criterion(preds, gts)
        return loss, None


class CLI(BaseMLCLI):
    def run_model(self, a):
        model = AttentionModel(num_features=3, params_count=10)
        x = torch.randn(3, 3)
        y = model(x)
        print(y.shape)

    class CommonArgs(BaseMLCLI.CommonArgs):
        epoch:int = 50
        overwrite: bool = Field(False, cli=('--overwrite', ))

        count:int = 3
        total:int = 1000

    class AttentionArgs(CommonArgs):
        pass

    def run_attention(self, a:AttentionArgs):
        config = AttentionTrainerConfig(
            num_features=a.count,
            total=a.total,
            count=a.count,
            loc_0=0.1,
            loc_1=0.8,
            scale=0.5,
        )
        ds = AttentionDataset(
            total=config.total,
            count=a.count,
            loc_0=config.loc_0,
            loc_1=config.loc_1,
            scale=config.scale,
        )
        trainer = Trainer(
            config=config,
            out_dir='out/models/attention',
            train_dataset=ds,
            overwrite=a.overwrite,
        )
        trainer.start(a.epoch)

    class LinearArgs(CommonArgs):
        pass

    def run_linear(self, a:LinearArgs):
        config = LinearTrainerConfig(
            num_features=a.count,
            total=a.total,
            count=a.count,
            loc_0=1.0,
            loc_1=2.0,
            scale=0.5,
        )
        ds = AttentionDataset(
            total=config.total,
            count=config.count,
            loc_0=config.loc_0,
            loc_1=config.loc_1,
            scale=config.scale,
        )
        trainer = LinearTrainer(
            config=config,
            out_dir='out/models/linear',
            train_dataset=ds,
            overwrite=a.overwrite,
        )
        trainer.start(a.epoch)

if __name__ == '__main__':
    cli = CLI()
    cli.run()
