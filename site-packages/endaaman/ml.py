import os
import re
import math
import random
from pathlib import Path
import subprocess
from typing import TypeVar
from functools import lru_cache

import torch
import numpy as np
import matplotlib
from matplotlib import ticker, pyplot as plt
from tqdm import tqdm
from pydantic import BaseModel, Field
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
import torch.multiprocessing
from torchvision import transforms

from .commander import Commander
from .trainer import Trainer, Checkpoint
from .predictor import Predictor
from .cli import BaseCLI
from .ml_utils import fix_global_seed, get_global_seed, pil_to_tensor, tensor_to_pil


torch.multiprocessing.set_sharing_strategy('file_system')

class MLCommander(Commander):
    def with_suffix(self, s):
        if self.a.suffix:
            return f'{s}_{self.a.suffix}'
        return s

    def _arg_common(self, parser):
        self._common_parser.add_argument('--seed', type=int, default=self.defaults.get('seed', get_global_seed()))
        self._common_parser.add_argument('--suffix', '-S', default='')
        super()._arg_common(parser)

    def _pre_common(self):
        fix_global_seed(self.a.seed)
        super()._pre_common()


class TorchCommander(MLCommander):
    def _pre_common(self):
        self.use_gpu = not self.a.cpu and torch.cuda.is_available()
        self.use_multi_gpu = self.use_gpu and torch.cuda.device_count() > 1
        self.device = torch.device('cuda' if self.use_gpu else 'cpu')
        self.mode = 'multi GPU' if self.use_multi_gpu else 'single GPU' if self.use_gpu else 'CPU'
        self.additional_info['mode'] = self.mode
        self.additional_info['device'] = self.device

        self.fmt_loss = '{:.4f}'
        super()._pre_common()

    def _arg_common(self, parser):
        # pylint: disable=unnecessary-lambda
        D = lambda name, value: self.defaults.get(name, value)
        parser.add_argument('--cpu', action='store_true')
        parser.add_argument('--batch-size', '-B', type=int, default=D('batch_size', 8))
        parser.add_argument('--num-workers', '-N', type=int, default=D('num_workers', 4))
        parser.add_argument('--lr', type=float, default=D('lr', 0.01))
        parser.add_argument('--epoch', '-E', type=int, default=D('epoch', 50))
        parser.add_argument('--log-dir', default=D('log_dir', 'runs'))
        parser.add_argument('--save', type=int, default=D('save_period', -1))
        super()._arg_common(parser)


    def as_loader(self, dataset, shuffle=True, **kwargs):
        return DataLoader(
            dataset=dataset,
            batch_size=self.a.batch_size,
            num_workers=self.a.num_workers,
            shuffle=shuffle,
            **kwargs,
        )

    def as_loaders(self, train_dataset, test_dataset, **kwargs):
        return [
            self.as_loader(dataset=train_dataset, **kwargs),
            self.as_loader(dataset=test_dataset, shuffle=False, **kwargs),
        ]

    def create_trainer(self, T, model_name, loaders, trainer_name=None, experiment_name=None, log_dir=None, **kwargs):
        return T(
            model_name=model_name,
            loaders=loaders,
            experiment_name=experiment_name,
            trainer_name=self.with_suffix(trainer_name or model_name),
            lr=self.a.lr,
            log_dir=log_dir or self.a.log_dir,
            device=self.device,
            save_period=self.a.save,
            **kwargs,
        )

    def create_predictor(self, P, checkpoint, **kwargs):
        return P(
            checkpoint=checkpoint,
            device=self.device,
            batch_size=self.a.batch_size,
            **kwargs,
        )

class BaseMLArgs(BaseModel):
    def __init__(self, **data):
        super().__init__(**data)
        fix_global_seed(self.seed)

    def with_suffix(self, s):
        if self.suffix:
            return f'{s}_{self.suffix}'
        return s


def define_ml_args(**D):
    class MLArgs(BaseMLArgs):
        seed: int = Field(D.pop('seed', get_global_seed()), cli=('--seed', ))
        suffix: str = Field('', cli=('--suffix', ))
    assert len(D) == 0, f'Unnecessary params included: {D}'
    return MLArgs

def define_torch_args(**D):
    class TorchArgs(BaseMLArgs):
        seed: int = Field(D.pop('seed', get_global_seed()), cli=('--seed', ))
        suffix: str = Field('', cli=('--suffix', ))

        experiment_name:str = Field('', cli=('--exp', ))
        cpu: bool = Field(False, cli=('--cpu', ))
        batch_size: int = Field(D.pop('batch_size', 8), cli=('--batch-size', '-B'))
        num_workers: int = Field(D.pop('num_workers', 4), cli=('--num_workers', '-N'))
        lr: float = Field(D.pop('lr'), cli=('--lr', ))
        epoch: int = Field(D.pop('epoch', 50), cli=('--epoch', '-E'))
        log_dir: str = Field('runs', cli=('--log-dir', ))
        save_period: int = Field(-1, cli=('--save', ))

        @property
        def using_gpu(self):
            return not self.cpu and torch.cuda.is_available()

        @property
        def using_multi_gpu(self):
            return not self.using_gpu and torch.cuda.device_count() > 1

        @property
        def device(self):
            return torch.device('cuda' if self.using_gpu else 'cpu')

        def as_loader(self, dataset, shuffle=True, **kwargs):
            return DataLoader(
                dataset=dataset,
                batch_size=self.batch_size,
                num_workers=self.num_workers,
                shuffle=shuffle,
                **kwargs,
            )

        def as_loaders(self, train_dataset, test_dataset, **kwargs):
            return [
                self.as_loader(dataset=train_dataset, **kwargs),
                self.as_loader(dataset=test_dataset, shuffle=False, **kwargs),
            ]

        def create_trainer(self, TrainerClass, model_name, loaders, experiment_name=None, trainer_name=None, extra=None):
            return TrainerClass(
                model_name,
                loaders,
                experiment_name=experiment_name or self.experiment_name,
                trainer_name=self.with_suffix(trainer_name or model_name),
                log_dir=self.log_dir,
                lr=self.lr,
                device=self.device,
                save_period=self.save_period,
                extra=extra
            )

    assert len(D) == 0, f'Unnecessary params included: {D}'
    return TorchArgs
