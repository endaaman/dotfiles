import os
import math
import random
from pathlib import Path
import subprocess

import torch
import numpy as np
import matplotlib
from matplotlib import ticker, pyplot as plt
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm

from .commander import Commander


def get_state_dict(model):
    if type(model) == torch.nn.DataParallel:
        return model.module.state_dict()
    return model.state_dict()

class TorchCommander(Commander):
    def with_suffix(self, s):
        if self.args.suffix:
            return f'{s}_{self.args.suffix}'
        return s

    def _arg_common(self, parser):
        self._common_parser.add_argument('--cpu', action='store_true')
        self._common_parser.add_argument('--seed', type=int, default=self.defaults.get('seed', 42))
        self._common_parser.add_argument('--suffix', )
        super()._arg_common(parser)

    def _pre_common(self):
        self.use_gpu = not self.args.cpu and torch.cuda.is_available()
        self.use_multi_gpu = self.use_gpu and torch.cuda.device_count() > 1
        self.device = torch.device('cuda' if self.use_gpu else 'cpu')
        self.mode = 'multi GPU' if self.use_multi_gpu else 'single GPU' if self.use_gpu else 'CPU'
        np.random.seed(self.args.seed)
        random.seed(self.args.seed)
        torch.manual_seed(self.args.seed)
        torch.cuda.manual_seed(self.args.seed)
        self.additional_info['mode'] = self.mode
        super()._pre_common()


class Trainer(TorchCommander):
    def _pre_common(self):
        self.fmt_loss = '{:.4f}'
        self.mean_metrics = False
        super()._pre_common()

    def _arg_common(self, parser):
        D = lambda name, value: self.defaults.get(name, value)
        parser.add_argument('--num-workers', type=int, default=D('num_workers', 4))
        parser.add_argument('-b', '--batch-size', type=int, default=D('batch_size', 128))
        parser.add_argument('--lr', type=float, default=D('lr', 0.01))
        parser.add_argument('-e', '--epoch', type=int, default=D('epoch', 30))
        parser.add_argument('--first-record-epoch', type=int, default=0)
        parser.add_argument('--period-save-weight', type=int, default=10)
        parser.add_argument('--no-show-fig', action='store_true')
        super()._arg_common(parser)

    def save_state(self, name, model, epoch, train_history, val_history, data=None):
        state = {
            'name': name,
            'epoch': epoch,
            'args': self.args,
            'state_dict': get_state_dict(model),
            'train_history': train_history,
            'val_history': val_history,
        }
        weights_dir = os.path.join('out', name, 'weights')
        os.makedirs(weights_dir, exist_ok=True)
        weights_path = os.path.join(weights_dir, f'{epoch}.pth')
        torch.save(state, weights_path)
        return weights_path

    def as_loader(self, ds):
        return DataLoader(
            ds,
            batch_size=self.args.batch_size,
            num_workers=self.args.num_workers,
            shuffle=True,
        )

    def train_model(self, name, model, train_loader, val_loader, eval_fn,
                    batch_metrics=None, epoch_metrics=None,
                    optimizer=None, scheduler_fn=None):
        if self.args.no_show_fig:
            matplotlib.use('Agg')
        print('Start training')

        out_dir = os.path.join('out', name)
        os.makedirs(out_dir, exist_ok=True)

        if not optimizer:
            optimizer = optim.Adam(model.parameters(), lr=self.args.lr)

        if scheduler_fn:
            scheduler = scheduler_fn(optimizer)
        else:
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)

        batch_metrics_fns = batch_metrics or {}
        epoch_metrics_fns = epoch_metrics or {}
        epoch_metrics_fns.update(batch_metrics_fns)
        batch_metrics_keys = ['loss', *batch_metrics_fns.keys(),]
        epoch_metrics_keys = ['loss', *batch_metrics_fns.keys(), *epoch_metrics.keys()]

        train_history = {k:[] for k in epoch_metrics_keys}
        val_history = {k:[] for k in epoch_metrics_keys}

        for epoch in range(1, self.args.epoch + 1):
            header = f'[{epoch}/{self.args.epoch}] '
            # ReduceLROnPlateau
            lr = optimizer.param_groups[0]['lr']
            # LambdaLR
            # lr = scheduler.get_last_lr()[0]
            print(f'{header}Starting lr={lr:.7f}')

            train_metrics = {k:[] for k in batch_metrics_keys}

            t = tqdm(train_loader, leave=False)
            outputss = []
            labelss = []

            model.eval()
            for (inputs, labels) in t:
                optimizer.zero_grad()
                loss, outputs = eval_fn(inputs, labels)
                loss.backward()
                optimizer.step()
                loss_value = float(loss.item())

                # cache to cpu
                outputs = outputs.detach().cpu()
                labels = labels.detach().cpu()
                outputss.append(outputs)
                labelss.append(labels)

                # add loss to metrics table
                train_metrics['loss'].append(loss_value)

                # calc normal metrics
                batch_metrics_messages = []
                for k, metrics_fn in batch_metrics_fns.items():
                    v = metrics_fn(outputs, labels)
                    train_metrics[k].append(v)
                    batch_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

                # build message
                batch_message = f'loss:{self.fmt_loss.format(loss_value)} ' + \
                    ' '.join(batch_metrics_messages)

                t.set_description(f'{header}{batch_message}')
                t.refresh()

            epoch_loss = np.mean(train_metrics['loss'])

            # add loss manually
            train_history['loss'].append(epoch_loss)

            # gather values
            epoch_outputs = torch.cat(outputss)
            epoch_labels = torch.cat(labelss)

            train_metrics_messages = []
            for k, metrics_fn in epoch_metrics_fns.items():
                vv = train_metrics.get(k)

                # re-use metrics
                if vv and self.mean_metrics:
                    v = np.mean(vv)
                else:
                    v = metrics_fn(epoch_outputs, epoch_labels)
                train_history[k].append(v)
                train_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

            # build message
            train_message = f'loss:{self.fmt_loss.format(epoch_loss)} ' + ' '.join(train_metrics_messages)
            print(f'{header}train: {train_message}')

            #* validate
            val_metrics = {k:[] for k in batch_metrics_keys}
            if val_loader:
                model.eval()
                outputss = []
                labelss = []
                with torch.set_grad_enabled(False):
                    for i, (inputs, labels) in enumerate(val_loader):
                        loss, outputs = eval_fn(inputs, labels)

                        # cache to cpu
                        outputs = outputs.detach().cpu()
                        labels = labels.detach().cpu()
                        outputss.append(outputs)
                        labelss.append(labels)

                        loss_value = float(loss.item())
                        val_metrics['loss'].append(loss_value)

                        for k, metrics_fn in batch_metrics.items():
                            val_metrics[k].append(metrics_fn(outputs, labels))


                mean_loss = np.mean(val_metrics['loss'])

                # add loss manually
                val_history['loss'].append(mean_loss)

                epoch_outputs = torch.cat(outputss)
                epoch_labels = torch.cat(labelss)

                val_metrics_messages = []
                for k, metrics_fn in epoch_metrics_fns.items():
                    vv = val_metrics.get(k)

                    # re-use metrics
                    if vv and self.mean_metrics:
                        v = np.mean(vv)
                    else:
                        v = metrics_fn(epoch_outputs, epoch_labels)
                    val_history[k].append(v)
                    val_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

                val_message = f'loss:{self.fmt_loss.format(mean_loss)} ' + ' '.join(val_metrics_messages)
                print(f'{header}val: {val_message}')

            #* draw fig
            if epoch > 1:
                fisrt_idx = self.args.first_record_epoch
                x_axis = np.arange(1, epoch+1)[fisrt_idx:]
                l = len(train_history.keys())
                max_col = 3
                max_row = math.ceil(l / max_col)
                fig = plt.figure(figsize=(10, 4 * max_row))
                for i, (k, train_values) in enumerate(train_history.items()):
                    ax = fig.add_subplot(max_row, max_col, i+1)
                    ax.get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))
                    ax.set_title(k)
                    ax.grid(axis='y')
                    ax.plot(x_axis, train_values[fisrt_idx:], label='train')
                    if val_loader:
                        val_values = val_history[k]
                        ax.plot(x_axis, val_values[fisrt_idx:], label='val')
                    ax.legend()
                fig_path = os.path.join(out_dir, 'curve.png')
                plt.savefig(fig_path)

                if epoch == 2 and not self.args.no_show_fig:
                    subprocess.run(['/usr/bin/xdg-open', fig_path])
                fig.clf()
                plt.clf()
                plt.close()

            #* save weights
            if epoch % self.args.period_save_weight == 0:
                weights_path = self.save_state(name, model, epoch, train_history, val_history)
                print(f'{header}Saved "{weights_path}"')

            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(train_metrics['loss'][-1])
            else:
                scheduler.step()
            print()


def pil_to_tensor(img):
    return transforms.functional.to_tensor(img)

def tensor_to_pil(tensor):
    a = tensor.min()
    b = tensor.max()
    img = (tensor - a) / (b - a)
    return transforms.functional.to_pil_image(img)

def calc_mean_and_std(images, dim=()):
    mean = None
    std = None
    to_tensor = transforms.ToTensor()
    for img in images:
        x = to_tensor(img)
        m, s = x.mean(dim=dim), x.std(dim=dim)
        mean = mean + m if type(mean) != type(None) else m
        std = std + s if type(std) != type(None) else s
    mean /= len(images)
    std /= len(images)
    return mean, std
