import os
import random
from pathlib import Path

import torch
import numpy as np
import matplotlib
from matplotlib import ticker, pyplot as plt
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from tqdm import tqdm

from .commander import Commander


def get_state_dict(model):
    if type(model) == torch.nn.DataParallel:
        return model.module.state_dict()
    else:
        return model.state_dict()

class TorchCommander(Commander):
    def with_suffix(self, s):
        if self.args.suffix:
            return f'{s}_{self.args.suffix}'
        return s

    def _arg_common(self, parser):
        self._common_parser.add_argument('--cpu', action='store_true')
        self._common_parser.add_argument('--seed', type=int, default=self.defaults.get('seed', 42))
        self._common_parser.add_argument('--suffix', )
        super()._arg_common(parser)

    def _pre_common(self):
        self.use_gpu = not self.args.cpu and torch.cuda.is_available()
        self.use_multi_gpu = self.use_gpu and torch.cuda.device_count() > 1
        self.device = torch.device('cuda' if self.use_gpu else 'cpu')
        self.mode = 'multi GPU' if self.use_multi_gpu else 'single GPU' if self.use_gpu else 'CPU'
        np.random.seed(self.args.seed)
        random.seed(self.args.seed)
        torch.manual_seed(self.args.seed)
        torch.cuda.manual_seed(self.args.seed)
        self.additional_info['mode'] = self.mode
        super()._pre_common()


class Trainer(TorchCommander):
    def _arg_common(self, parser):
        D = lambda name, value: self.defaults.get(name, value)
        parser.add_argument('--num-workers', type=int, default=D('num_workers', 4))
        parser.add_argument('-b', '--batch-size', type=int, default=D('batch_size', 128))
        parser.add_argument('--lr', type=float, default=D('lr', 0.01))
        parser.add_argument('-e', '--epoch', type=int, default=D('epoch', 30))
        parser.add_argument('--first-record-epoch', type=int, default=0)
        parser.add_argument('--period-save-weight', type=int, default=10)
        parser.add_argument('--no-show-fig', action='store_true')
        super()._arg_common(parser)

    def save_state(self, name, model, epoch, train_history, val_history, data=None):
        state = {
            'name': name,
            'epoch': epoch,
            'args': self.args,
            'state_dict': get_state_dict(model),
            'train_history': train_history,
            'val_history': val_history,
        }
        weights_dir = os.path.join('out', name, 'weights')
        os.makedirs(weights_dir, exist_ok=True)
        weights_path = os.path.join(weights_dir, f'{epoch}.pth')
        torch.save(state, weights_path)
        return weights_path

    def as_loader(self, ds):
        return DataLoader(
            ds,
            batch_size=self.args.batch_size,
            num_workers=self.args.num_workers,
            shuffle=True,
        )

    def train_model(self, name, model, train_loader, val_loader, eval_fn, batch_metrics={}, epoch_metrics={}):
        if self.args.no_show_fig:
            matplotlib.use('Agg')
        out_dir = os.path.join('out', name)
        os.makedirs(out_dir, exist_ok=True)

        optimizer = optim.Adam(model.parameters(), lr=self.args.lr)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)
        # def lr_func_exp(step):
        #     return 0.90 ** step
        # scheduler = LambdaLR(optimizer, lr_lambda=lambda x: 0.99 ** x)

        print('Starting training')

        batch_metrics_keys = ['loss', *batch_metrics.keys(),]
        epoch_metrics_keys = ['loss', *batch_metrics.keys(), *epoch_metrics.keys()]

        train_history = {k:[] for k in epoch_metrics_keys}
        val_history = {k:[] for k in epoch_metrics_keys}

        for epoch in range(1, self.args.epoch + 1):
            header = f'[{epoch}/{self.args.epoch}] '
            # ReduceLROnPlateau
            lr = optimizer.param_groups[0]['lr']
            # LambdaLR
            # lr = scheduler.get_last_lr()[0]
            print(f'{header}Starting lr={lr:.7f}')

            train_metrics = {k:[] for k in batch_metrics_keys}

            t = tqdm(train_loader, leave=False)
            labelss = []
            outputss = []
            for (inputs, labels) in t:
                optimizer.zero_grad()
                loss, outputs = eval_fn(inputs, labels)
                loss.backward()
                optimizer.step()
                train_metrics['loss'].append(float(loss.item()))
                # outputs = outputs.cpu().detach()
                for k, metrics_fn in batch_metrics.items():
                    v = metrics_fn(outputs, labels)
                    train_metrics[k].append(v)
                message = ' '.join([f'{k}:{v[-1]:.4f}' for k, v in train_metrics.items()])
                labelss.append(labels.to('cpu').detach())
                outputss.append(outputs.to('cpu').detach())
                t.set_description(f'{header}{message}')
                t.refresh()

            for k, v in train_metrics.items():
                train_history[k].append(np.mean(v))
            epoch_outputs = torch.cat(outputss)
            epoch_labels = torch.cat(labelss)
            for k, metrics_fn in epoch_metrics.items():
                train_history[k].append(metrics_fn(epoch_outputs, epoch_labels))
            train_message = ' '.join([f'{k}:{v[-1]:.4f}' for k, v in train_history.items()])
            print(f'{header}train: {train_message}')

            #* validate
            val_metrics = {k:[] for k in batch_metrics_keys}
            if val_loader:
                model.eval()
                labelss = []
                outputss = []
                with torch.set_grad_enabled(False):
                    for i, (inputs, labels) in enumerate(val_loader):
                        loss, outputs = eval_fn(inputs, labels)
                        # outputs = outputs.cpu().detach()
                        val_metrics['loss'].append(float(loss.item()))
                        for k, metrics_fn in batch_metrics.items():
                            val_metrics[k].append(metrics_fn(outputs, labels))
                        outputss.append(outputs)
                        labelss.append(labels)
                model.train()
                for k, v in val_metrics.items():
                    val_history[k].append(np.mean(v))
                epoch_outputs = torch.cat(outputss)
                epoch_labels = torch.cat(labelss)
                for k, metrics_fn in epoch_metrics.items():
                    val_history[k].append(metrics_fn(epoch_outputs, epoch_labels))
                val_message = ' '.join([f'{k}:{v[-1]:.4f}' for k, v in val_history.items()])
                print(f'{header}val: {val_message}')

            #* draw fig
            if epoch > 1:
                fisrt_idx = self.args.first_record_epoch
                x_axis = np.arange(1, epoch+1)[fisrt_idx:]
                fig = plt.figure(figsize=(10, 5))
                for i, (k, train_values) in enumerate(train_history.items()):
                    ax = fig.add_subplot(1, len(train_history.keys()), i+1)
                    ax.get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))
                    ax.set_title(k)
                    ax.grid(axis='y')
                    ax.plot(x_axis, train_values[fisrt_idx:], label=f'train')
                    if val_loader:
                        val_values = val_history[k]
                        ax.plot(x_axis, val_values[fisrt_idx:], label=f'val')
                    ax.legend()
                fig_path = os.path.join(out_dir, 'curve.png')
                plt.savefig(fig_path)

                if epoch == 2 and not self.args.no_show_fig:
                    os.system(f'xdg-open {fig_path} > /dev/null')
                fig.clf()
                plt.clf()
                plt.close()

            #* save weights
            if epoch % self.args.period_save_weight == 0:
                weights_path = self.save_state(name, model, epoch, train_history, val_history)
                print(f'{header}Saved "{weights_path}"')

            scheduler.step(train_metrics['loss'][-1])
            # scheduler.step()
            print()
