import os
import re
import math
import sys
import random
import subprocess
import json
from abc import ABCMeta, abstractmethod
from typing import NamedTuple
from collections import OrderedDict
from datetime import datetime

import numpy as np
import matplotlib
from matplotlib import ticker, pyplot as plt
import torch
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader, Dataset
# from torch.utils.tensorboard import SummaryWriter
from tensorboardX import SummaryWriter
import mlflow
from torchvision import transforms
from torchvision.utils import make_grid
from tqdm import tqdm
import pydantic
from pydantic import BaseModel

from .utils import wrap_iter, yes_no_input
from .metrics import BaseMetrics
from .ml_utils import restore_random_states, get_random_states, smart_cat


J = os.path.join
CONFIG_FILE = 'config.json'
LAST_CHECKPOINT = 'checkpoint_last.pt'
BEST_CHECKPOINT = 'checkpoint_best.pt'


class BaseTrainerConfig(BaseModel):
    batch_size:int
    num_workers: int
    lr: float

    class Config:
        extra = pydantic.Extra.forbid
        validate_assignment = True
        frozen = True


class Checkpoint(NamedTuple):
    config: dict
    epoch: int
    model_state: dict
    optimizer_state: dict
    scheduler_state: dict
    train_history: dict
    val_history: dict
    random_states: dict


class BaseTrainer(metaclass=ABCMeta):
    def __init__(self,
                 config:BaseTrainerConfig,
                 out_dir:str,
                 train_dataset:Dataset,
                 val_dataset:Dataset = None,
                 use_gpu = True,
                 loss_fmt = '{:3f}',
                 main_metrics = 'loss',
                 ):
        self.config = config
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.use_gpu = use_gpu
        self.loss_fmt = loss_fmt

        self.scheduler = None
        self.optimizer = None
        self.writer = None
        self.train_loader = None
        self.val_loader = None
        self.device = torch.device('cuda' if self.use_gpu and torch.cuda.is_available() else 'cpu')
        self.model = self.prepare()

        self.optimizer = self.create_optimizer()
        self.scheduler = self.create_scheduler()

        self.metrics:dict[str, BaseMetrics] = {}
        self.metrics.update(self.get_metrics())
        self.train_history = {k:[] for k in ['loss', *self.metrics.keys()]}
        self.val_history = {k:[] for k in ['loss', *self.metrics.keys()]}

        for (ds, is_train) in ((train_dataset, True), (val_dataset, False)):
            l = DataLoader(
                dataset=ds,
                batch_size=config.batch_size,
                num_workers=config.num_workers,
                shuffle=is_train,
            )
            if is_train:
                self.train_loader = l
            else:
                self.val_loader = l


        # state
        self.current_epoch = 1
        self.best_metric = -1
        self.checkpoint = None

        # select out dir
        chp_path = self.select_out_dir(out_dir)
        if chp_path:
            print(f'Automatically restoring from {chp_path}')
            self.restore(checkpoint=torch.load(chp_path))

        # write config before run
        config_file_path = J(self.out_dir, CONFIG_FILE)
        with open(config_file_path, 'w', encoding='utf-8') as f:
            f.write(self.config.json(indent=2))
        print(f'wrote {config_file_path}')


    def select_out_dir(self, out_dir):
        # returns needs to restore checkpoint
        if not os.path.isdir(out_dir):
            self.out_dir = out_dir
            os.makedirs(self.out_dir, exist_ok=True)
            return None

        config_file_path = J(out_dir, CONFIG_FILE)
        if not os.path.exists(config_file_path):
            self.out_dir = out_dir
            print(f'Out dir `{out_dir}` exists but, theres is no config file.')
            return None

        with open(config_file_path, 'r', encoding='utf-8') as f:
            old_config = self.config.__class__(**json.load(f))

        if old_config == self.config:
            self.out_dir = out_dir
            chp_path = J(self.out_dir, LAST_CHECKPOINT)
            if os.path.exists(chp_path):
                return chp_path
            print(f'Old checkpoint {chp_path} does not exist. Starting from the begining.')
            return None

        m = f'The old config {config_file_path} already exists but is different with the current config. \n' \
            f'[current]:\n{self.config}\n' \
            f'[old]    :\n{old_config}\n'
        print(m)
        if yes_no_input('Overwrite this?'):
            os.unlink(chp_path)
            self.out_dir = out_dir
            return None

        if yes_no_input('Set new sequential number?'):
            i = 0
            while True:
                new_out_dir = f'{out_dir}_{i}'
                if not os.path.exists(new_out_dir):
                    break
                i += 1
            self.out_dir = new_out_dir
            os.makedirs(new_out_dir, exist_ok=True)
            print(f'New out dir: {new_out_dir}')
            return None

        suffix = input('Set suffix: ').lower()
        self.out_dir = out_dir + '_' + suffix
        return None


    @abstractmethod
    def prepare(self):
        # create model
        return None

    def get_model_state_dict(self):
        self.model.cpu()
        if isinstance(self.model, torch.nn.DataParallel):
            return self.model.module.state_dict()
        return self.model.state_dict()

    def create_optimizer(self):
        return optim.Adam(self.model.parameters(), lr=self.config.lr)

    def create_scheduler(self):
        # return optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=10)
        # return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda _: self.lr)
        return None

    def is_achieved_best(self):
        h = self.val_history if self.val_loader else self.train_history
        return np.argmin(h[self.main_metrics]) == len(h[self.main_metrics]) - 1

    def get_metrics(self):
        return { }

    def save_checkpoint(self, path):
        checkpoint = Checkpoint(
            config=self.config.dict(),
            epoch=self.current_epoch,
            model_state=self.get_model_state_dict(),
            optimizer_state=self.optimizer.state_dict(),
            scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
            train_history=self.train_history,
            val_history=self.val_history,
            random_states=get_random_states(),
        )
        torch.save(checkpoint, path)

    def hook_load_state(self, checkpoint):
        pass

    def step(self, train_loss):
        if self.scheduler is None:
            return
        if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            self.scheduler.step(train_loss)
            return
        # pylint: disable=no-value-for-parameter
        self.scheduler.step()

    def prepare_mlops(self):
        if not self.writer:
            tb_log_dir = J(self.out_dir, 'logs')
            self.writer = SummaryWriter(log_dir=tb_log_dir)
            print(f'using log dir: tensorboard=`{tb_log_dir}`')

    @abstractmethod
    def eval(self, inputs, gts):
        # DO:
        # - .to(self.device)
        # - return loss, preds
        # DO NOT:
        # - self.optimizer.zero_grad()
        # - loss.backward()
        pass

    def record_value(self, k, value, step):
        self.writer.add_scalar(k, value, step)

    def restore(self, checkpoint: Checkpoint):
        self.model.cpu()
        self.model.load_state_dict(checkpoint.model_state)
        self.optimizer.load_state_dict(checkpoint.optimizer_state)
        restore_random_states(checkpoint.random_states)
        data = (
            ('train', self.train_history, checkpoint.train_history),
            ('val', self.val_history, checkpoint.val_history)
        )
        self.prepare_mlops()
        for target, history, chp_history in data:
            for k, vv in chp_history.items():
                history[k] = vv
                for i, v in enumerate(vv):
                    epoch = i + 1
                    self.writer.add_scalar(f'{k}/{target}', v, epoch)
        self.writer.flush()
        self.hook_load_state(checkpoint)
        self.current_epoch = checkpoint.epoch + 1
        self.checkpoint = checkpoint


    def eval_loader(self, is_training, loader, history):
        if is_training:
            target = 'train'
            self.model.train()
        else:
            target = 'val'
            self.model.eval()

        losses = []
        t = tqdm(loader, leave=False)
        predss = []
        gtss = []
        for (inputs, gts) in t:
            self.model.to(self.device)
            if is_training:
                self.optimizer.zero_grad()
            loss, preds = self.eval(inputs, gts)

            if is_training:
                loss.backward()
                self.optimizer.step()
            loss_value = float(loss.item())
            losses.append(loss_value)

            # calc batch metrics
            batch_metrics_messages = []
            for k, m in self.metrics.items():
                v = m(preds, gts)
                v_str = m.format.format(v)
                batch_metrics_messages.append(f'{k}:{v_str}')
            # build message
            loss_str = self.loss_fmt.format(loss_value)
            t.set_description(f'[{target}] loss:{loss_str} ' + ' '.join(batch_metrics_messages))
            t.refresh()

            predss.append(preds)
            gtss.append(gts)

        epoch_loss = np.mean(losses)
        history['loss'].append(epoch_loss)

        # unite other metrics values
        metrics_messages = []
        # gather values
        epoch_preds = smart_cat(predss)
        epoch_gts = smart_cat(gtss)

        for k, m in self.metrics.items():
            v = m(epoch_preds, epoch_gts)
            history[k].append(v)
            v_str = m.format.format(v)
            metrics_messages.append(f'{k}:{v_str}')

        # build message
        loss_str = self.loss_fmt.format(epoch_loss)
        print(f'[{target}]: loss:{loss_str} ' + ' '.join(metrics_messages))


    def start(self, total_epoch):
        print(f'Start training [{self.current_epoch}/{total_epoch}]')
        # TODO: pretty print
        print(f'config: {self.config}')
        matplotlib.use('Agg')
        self.model.to(self.device)

        while self.current_epoch <= total_epoch:
            # ReduceLROnPlateau
            current_lr = self.optimizer.param_groups[0]['lr']
            # LambdaLR
            # lr = scheduler.get_last_lr()[0]

            now = datetime.now().time().strftime('%X')
            print(f'[{self.current_epoch}/{total_epoch}] Start training ({now})')

            self.eval_loader(True, self.train_loader, self.train_history)

            if self.val_loader:
                with torch.set_grad_enabled(False):
                    self.eval_loader(False, self.val_loader, self.val_history)

            #* draw fig
            if False and self.current_epoch > 1:
                x_axis = np.arange(1, self.current_epoch+1)
                count = len(self.train_history.keys())
                max_col = min(count, 3)
                max_row = math.ceil(count / max_col)
                # 1: 6, 2:8 ,3:10
                fig = plt.figure(figsize=(4+min(count, 3) * 3, 4 * max_row))
                for i, (k, train_values) in enumerate(self.train_history.items()):
                    ax = fig.add_subplot(max_row, max_col, i+1)
                    ax.get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))
                    ax.set_title(k)
                    ax.grid(axis='y')
                    ax.plot(x_axis, train_values, label='train')
                    if self.val_loader:
                        val_values = self.val_history[k]
                        ax.plot(x_axis, val_values, label='val')
                    ax.legend()
                fig_path = os.path.join(self.out_dir, 'curve.png')
                plt.savefig(fig_path)

                if self.current_epoch == 1:
                    print(f'wrote {fig_path}')
                fig.clf()
                plt.clf()
                plt.close()

            self.prepare_mlops()
            for (t, history) in (('train', self.train_history), ('val', self.val_history)):
                for k, values in history.items():
                    self.record_value(f'{k}/{t}', values[-1], self.current_epoch)
            self.record_value('lr', current_lr, self.current_epoch)
            self.writer.flush()

            self.save_checkpoint(J(self.out_dir, LAST_CHECKPOINT))
            if self.is_achieved_best():
                self.save_checkpoint(J(self.out_dir, BEST_CHECKPOINT))
                print('save best checkpoint')

            self.step(self.train_history['loss'][-1])
            self.current_epoch += 1
            print()

        self.writer.close()
        mlflow.end_run()
        print('done.')
