import os
import re
import math
import random
import subprocess
import json
from abc import ABCMeta, abstractmethod
from typing import NamedTuple
from collections import OrderedDict
from datetime import datetime

import numpy as np
import matplotlib
from matplotlib import ticker, pyplot as plt
import torch
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
# from torch.utils.tensorboard import SummaryWriter
from tensorboardX import SummaryWriter
import mlflow
from torchvision import transforms
from torchvision.utils import make_grid
from tqdm import tqdm
import pydantic
from pydantic import BaseConfig

from .utils import wrap_iter
from .metrics import BaseMetrics
from .ml_utils import restore_random_states, get_random_states, smart_cat


DEFAULT_FORMAT = '{.3f}'
J = os.path.join
CONFIG_FILE = 'config.json'
LAST_CHECKPOINT = 'checkpoint_last.pt'
BEST_CHECKPOINT = 'checkpoint_best.pt'


class TrainerConfig(BaseConfig):
    save_dir: str = 'models'
    model_name: str
    experiment_name: str
    batch_size:int = 4
    num_workers: int = 4
    lr: float

    class Config:
        extra = pydantic.Extra.forbid
        validate_assignment = True
        frozen = True


class Checkpoint(NamedTuple):
    config: dict
    epoch: int
    model_state: dict
    optimizer_state: dict
    scheduler_state: dict
    train_history: dict
    val_history: dict
    random_states: dict


class BaseTrainer(metaclass=ABCMeta):
    def __init__(self,
                 config: TrainerConfig,
                 train_dataset,
                 val_dataset = None,
                 ):
        self.config = config
        self.scheduler = None
        self.optimizer = None
        self.writer = None
        self.model = self.prepare(config)

        self.metrics:dict[str, BaseMetrics] = {}
        self.metrics.update(self.get_metrics())
        self.train_history = {k:[] for k in ['loss', *self.metrics.keys()]}
        self.val_history = {k:[] for k in ['loss', *self.metrics.keys()]}

        # select out dir
        self.out_dir = J(self.config.save_dir, self.config.model_name)
        os.makedirs(self.out_dir, exist_ok=True)

        # state
        self.current_epoch = 1
        self.best_loss = -1
        self.checkpoint = None

        # auto restore
        past_config_file = J(self.out_dir, CONFIG_FILE)
        if os.path.exists(past_config_file):
            with open(past_config_file, 'r', encoding='utf-8') as f:
                past_config = self.config.__class__(**json.load(f))
                if past_config != self.config:
                    m = f'The name {out_dir} already exists, and differenct config. \n' \
                        f'current config:\n{self.config}\n' \
                        f'past config:\n{past_config}\n'
                    raise RuntimeError(m)
                chp_path = J(self.out_dir, LAST_CHECKPOINT)
                self.restore(checkpoint=torch.load(chp_path))
                print(f'Automatically restored {chp_path}')

    @abstractmethod
    def prepare(self):
        return None

    def get_model_state_dict(self):
        if isinstance(self.model, torch.nn.DataParallel):
            return self.model.module.state_dict()
        return self.model.state_dict()

    def create_optimizer(self):
        return optim.Adam(self.model.parameters(), lr=self.lr)

    def create_scheduler(self, total_epoch):
        # return optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=10)
        # return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda _: self.lr)
        return None

    def get_loss_fmt(self):
        return DEFAULT_FORMAT

    def get_metrics(self):
        return { }

    def save_checkpoint(self, path):
        checkpoint = Checkpoint(
            config=self.config,
            epoch=self.current_epoch,
            model_state=self.get_model_state_dict(),
            optimizer_state=self.optimizer.state_dict(),
            scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
            train_history=self.train_history,
            val_history=self.val_history,
            random_states=get_random_states(),
        )
        torch.save(checkpoint, path)

    def hook_load_state(self, checkpoint):
        pass

    def step(self, train_loss):
        if self.scheduler is None:
            return
        if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            self.scheduler.step(train_loss)
            return
        # pylint: disable=no-value-for-parameter
        self.scheduler.step()

    def prepare_mlops(self):
        if not self.writer:
            tb_log_dir = J(self.out_dir, 'logs')
            self.writer = SummaryWriter(log_dir=tb_log_dir)
            print(f'using log dir: tensorboard=`{tb_log_dir}`')

    @abstractmethod
    def eval(self, inputs, gts):
        # 以下は外で
        # self.optimizer.zero_grad()
        # loss.backward()
        # return loss, preds
        pass

    def record_value(self, k, value, step):
        self.writer.add_scalar(k, value, step)

    def restore(self, checkpoint):
        self.model.load_state_dict(checkpoint.model_state)
        self.optimizer.load_state_dict(checkpoint.optimizer_state)
        restore_random_states(checkpoint.random_states)
        data = (
            ('train', self.train_history, checkpoint.train_history),
            ('val', self.val_history, checkpoint.val_history)
        )
        self.prepare_mlops()
        for target, history, chp_history in data:
            for k, vv in chp_history.items():
                history[k] = vv
                for i, v in enumerate(vv):
                    epoch = i + 1
                    self.writer.add_scalar(f'{k}/{target}', v, epoch)
        self.writer.flush()
        self.hook_load_state(checkpoint)
        self.current_epoch = checkpoint.epoch + 1
        self.checkpoint = checkpoint


    def eval_loader(self, train:bool, loader, history):
        if train:
            self.model.train()
        else:
            self.model.eval()

        losses = []
        t = tqdm(loader)
        predss = []
        gtss = []
        for (inputs, gts) in t:
            if train:
                self.optimizer.zero_grad()
            loss, preds = self.eval(inputs, gts)

            if train:
                loss.backward()
                self.optimizer.step()
            loss_value = float(loss.item())
            losses.append(loss_value)

            # TODO: zipなどできるように
            preds = preds.detach().cpu()
            gts = gts.detach().cpu()

            # calc batch metrics
            batch_metrics_messages = []
            for k, m in self.metrics.items():
                v = m(preds, gts)
                v_str = m.format.format(v)
                batch_metrics_messages.append(f'{k}:{v_str}')
            # build message
            loss_str = self.get_loss_fmt().format(loss_value)
            t.set_description(f'loss:{loss_str} ' + ' '.join(batch_metrics_messages))
            t.refresh()

            predss.append(preds)
            gtss.append(gts)

        epoch_loss = np.mean(losses)
        self.history['loss'].append(epoch_loss)

        # unite other metrics values
        train_metrics_messages = []
        # gather values
        epoch_preds = smart_cat(predss)
        epoch_gts = smart_cat(gtss)

        for k, m in self.epoch_metrics.items():
            v = metrics_fn(epoch_preds, epoch_gts)
            self.train_history[k].append(v)
            v_str = m.format.format(v)
            train_metrics_messages.append(f'{k}:{v_str}')

        # build message
        loss_str = self.get_loss_fmt().format(epoch_loss)
        train_message = f'loss:{loss_str} ' + ' '.join(train_metrics_messages)
        print(f'{target}: {train_message}')


    def start(self, total_epoch):
        print('Start training')
        matplotlib.use('Agg')

        self.model.to(self.device)
        train_loader, val_loader = self.loaders
        self.optimizer = self.create_optimizer()
        self.scheduler = self.create_scheduler(total_epoch)

        out_dir = self.get_out_dir()
        os.makedirs(out_dir, exist_ok=True)

        while self.current_epoch <= total_epoch:
            self.current_header = f'[{self.current_epoch}/{total_epoch}] '
            # ReduceLROnPlateau
            current_lr = self.optimizer.param_groups[0]['lr']
            # LambdaLR
            # lr = scheduler.get_last_lr()[0]

            now = datetime.now().time().strftime('%X')
            print(f'{header}Starting lr={current_lr:.7f} ({now})')

            train_metrics_values = {k:[] for k in self.train_history.keys()}

            t = tqdm(train_loader, leave=False)
            predss = []
            gtss = []

            #* train
            self.train_once()

            #* validate
            val_metrics_values = {k:[] for k in self.val_history.keys()}
            if val_loader:
                self.model.eval()
                predss = []
                gtss = []
                # images = []
                with torch.set_grad_enabled(False):
                    t = tqdm(enumerate(val_loader), leave=False, total=len(val_loader))
                    t.set_description(f'{header}Evaluating validation dataset..')
                    for i, (inputs, gts) in t:
                        loss, preds = self.eval(inputs, gts)

                        # if self.report_image:
                        #     images += self.eval_image(inputs, gts, preds)

                        loss_value = float(loss.item())
                        val_metrics_values['loss'].append(loss_value)

                        if not self.no_metrics:
                            # cache to cpu
                            preds = preds.detach().cpu()
                            gts = gts.detach().cpu()
                            predss.append(preds)
                            gtss.append(gts)
                            for k, metrics_fn in self.batch_metrics.items():
                                val_metrics_values[k].append(metrics_fn(preds, gts))

                val_epoch_loss = np.mean(val_metrics_values['loss'])

                # add loss manually
                self.val_history['loss'].append(val_epoch_loss)

                val_metrics_messages = []
                if not self.no_metrics:
                    epoch_preds = smart_cat(predss)
                    epoch_gts = smart_cat(gtss)

                    for k, metrics_fn in self.epoch_metrics.items():
                        vv = val_metrics_values.get(k)
                        # re-use metrics
                        v = metrics_fn(epoch_preds, epoch_gts)
                        self.val_history[k].append(v)
                        m.format.format(v)
                        val_metrics_messages.append(f'{k}:{v:{fmt}}')

                val_message = f'loss:{val_epoch_loss:{self.get_loss_fmt()}} ' + ' '.join(val_metrics_messages)
                print(f'{header}val: {val_message}')


            #* draw fig
            if epoch > 1:
                x_axis = np.arange(1, epoch+1)
                count = len(self.train_history.keys())
                max_col = min(count, 3)
                max_row = math.ceil(count / max_col)
                # 1: 6, 2:8 ,3:10
                fig = plt.figure(figsize=(4+min(count, 3) * 3, 4 * max_row))
                for i, (k, train_values) in enumerate(self.train_history.items()):
                    ax = fig.add_subplot(max_row, max_col, i+1)
                    ax.get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))
                    ax.set_title(k)
                    ax.grid(axis='y')
                    ax.plot(x_axis, train_values, label='train')
                    if val_loader:
                        val_values = self.val_history[k]
                        ax.plot(x_axis, val_values, label='val')
                    ax.legend()
                fig_path = os.path.join(out_dir, 'curve.png')
                plt.savefig(fig_path)

                if epoch == 1:
                    print(f'wrote {fig_path}')
                fig.clf()
                plt.clf()
                plt.close()

            #* tensorboard
            self.prepare_mlops()

            groups = {
                None: ['loss'],
            }
            for k, metrics_fn in self.epoch_metrics.items():
                kk = groups.get(metrics_fn.group)
                if kk:
                    kk.append(k)
                else:
                    groups[metrics_fn.group] = [k]
            # groups = [
            #    None: ['loss', 'auc'],
            #    'acc': ['acc', 'recall', 'spec'],
            # ]

            historys = [['train', self.train_history]]
            if val_loader:
                historys.append(['val', self.val_history])

            for (target, history) in historys:
                for group_name, group in groups.items():
                    if group_name:
                        values = {}
                        for k in group:
                            values[k] = history[k][-1]
                        self.writer.add_scalars(f'{group_name}/{target}', values, epoch)
                    else:
                        for k in group:
                            self.record_value(f'{k}/{target}', history[k][-1], epoch)

            self.record_value('lr', current_lr, epoch)
            # if self.report_image:
            #     pass
            #     image = self.merge_images(images)
            #     if image is not None and image.any():
            #         self.writer.add_image('image/val', image, epoch)

            self.writer.flush()

            #* save checkpoints
            if (self.save_period > 0 and epoch % self.save_period == 0) or (self.save_period <= 0 and epoch == total_epoch):
                chp_path = self.save_checkpoint(epoch)
                print(f'{header}Saved "{chp_path}"')

            self.step(train_epoch_loss)
            self.current_epoch += 1
            print()

        self.writer.close()
        mlflow.end_run()
        print('done.')
