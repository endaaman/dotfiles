import torch
import numpy as np
from sklearn import metrics


class MetricsFn:
    def __init__(self, fmt='{.f4}', threshold=0.5):
        self.fmt = fmt
        self.threshold = threshold

    def calc(self, outputs, labels):
        return -99.0

    def format(self, v):
        return self.fmt.format(v)


class BinaryAccuracy(MetricsFn):
    def caln(self, outputs, labels):
        pred_y = outputs.cpu().flatten() > self.threshold
        true_y = labels.cpu().flatten() > self.threshold
        correct = torch.sum(true_y == pred_y)
        return correct / len(true_y)

class BinaryRecall(MetricsFn):
    def calc(self, outputs, labels):
        pred_y = outputs.cpu().flatten() > self.threshold
        true_y = labels.cpu().flatten() > self.threshold
        pos_true_y = true_y[true_y > 0]
        if len(pos_true_y) == 0:
            return -0.1
        return torch.sum(pred_y[true_y > 0] == pos_true_y) / len(pos_true_y)

class BinarySpecificity(MetricsFn):
    def calc(self, outputs, labels):
        pred_y = outputs.cpu().flatten() > self.threshold
        true_y = labels.cpu().flatten() > self.threshold
        neg_true_y = true_y[true_y < 1]
        if len(neg_true_y) == 0:
            return -0.1
        return torch.sum(pred_y[true_y < 1] == neg_true_y) / len(neg_true_y)

class BinaryAUC(MetricsFn):
    def calc(self, outputs, labels):
        pred_y = outputs.cpu().flatten().numpy()
        true_y = labels.cpu().flatten().numpy()
        return metrics.roc_auc_score(true_y, pred_y)

class MultiAccuracy(MetricsFn):
    def __init__(self, by_index=True, **kwargs):
        super().__init__(self, **kwargs)
        self.by_index = by_index

    def calc(self, outputs, labels):
        pred_y = outputs.cpu()
        pred_idx = np.argmax(pred_y, axis=1)
        if self.by_index:
            true_idx = labels.cpu().numpy()
        else:
            true_idx = np.argmax(labels.cpu().numpy(), axis=1)

        correct = np.sum(true_idx == pred_idx)
        return correct / len(true_idx)
