import os
import re
import math
import random
import subprocess
from abc import ABCMeta, abstractmethod
from datetime import datetime

import torch
import numpy as np
import matplotlib
from matplotlib import ticker, pyplot as plt
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from tqdm import tqdm


class Trainer(metaclass=ABCMeta):
    def __init__(self, name, model, loaders):
        self.name = name
        self.model = model
        self.loaders = loaders
        self.writer = SummaryWriter(log_dir='./data/logs')
        self.prepare()

    def prepare(self):
        pass

    def get_mean_metrics(self):
        return []

    def get_out_dir(self):
        return os.path.join('out', self.name)

    def get_state_dict(self):
        if isinstance(self.model, torch.nn.DataParallel):
            return self.model.module.state_dict()
        return self.model.state_dict()

    def get_optimizer(self, lr):
        return optim.Adam(self.model.parameters(), lr=lr)

    def get_scheduler(self, optimizer):
        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)

    def get_metrics_fns(self, target):
        fns = {}
        for name in dir(self):
            if m := re.match('^metrics_' + target + r'_(.*)$', name):
                continue
            f = getattr(self, name)
            if not callable(f):
                continue
            fns[m[1]] = f
        return fns

    def get_loss_fmt(self):
        return '{:.4f}'

    def get_batch_metrics(self):
        return {}

    def get_epoch_metrics(self):
        return {}

    def save_state(self, epoch, train_history, val_history):
        state = {
            'name': self.name,
            'epoch': epoch,
            'state_dict': self.get_state_dict(),
            'train_history': train_history,
            'val_history': val_history,
        }
        weights_dir = os.path.join(self.get_out_dir(), 'weights')
        os.makedirs(weights_dir, exist_ok=True)
        weights_path = os.path.join(weights_dir, f'{epoch}.pth')
        torch.save(state, weights_path)
        return weights_path

    def step(self, scheduler, last_loss):
        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(last_loss)
        else:
            scheduler.step()

    @abstractmethod
    def eval(self, inputs, labels, device):
        # return loss, outputs
        pass

    def train(self, lr, max_epoch, device='cpu', save_period=10, show_fig=True, first_record_epoch=1):
        if not show_fig:
            matplotlib.use('Agg')
        print('Start training')
        self.model.to(device)

        train_loader, val_loader = self.loaders

        out_dir = self.get_out_dir()
        os.makedirs(out_dir, exist_ok=True)

        optimizer = self.get_optimizer(lr)
        scheduler = self.get_scheduler(optimizer)

        batch_metrics_fns = self.get_batch_metrics()
        epoch_metrics_fns = self.get_epoch_metrics()
        epoch_metrics_fns.update(batch_metrics_fns)
        batch_metrics_keys = ['loss', *batch_metrics_fns.keys(),]
        epoch_metrics_keys = ['loss', *batch_metrics_fns.keys(), *epoch_metrics_fns.keys()]

        no_metrics = len(batch_metrics_fns) + len(epoch_metrics_fns) == 0

        train_history = {k:[] for k in epoch_metrics_keys}
        val_history = {k:[] for k in epoch_metrics_keys}

        for epoch in range(1, max_epoch + 1):
            header = f'[{epoch}/{max_epoch}] '
            # ReduceLROnPlateau
            lr = optimizer.param_groups[0]['lr']
            # LambdaLR
            # lr = scheduler.get_last_lr()[0]

            now = datetime.now().time().strftime('%X')
            print(f'{header}Starting lr={lr:.7f} ({now})')

            train_metrics = {k:[] for k in batch_metrics_keys}

            t = tqdm(train_loader, leave=False)
            outputss = []
            labelss = []

            self.model.eval()
            for (inputs, labels) in t:
                optimizer.zero_grad()
                loss, outputs = self.eval(inputs, labels, device)
                loss.backward()
                optimizer.step()
                loss_value = float(loss.item())

                # add loss to metrics table
                train_metrics['loss'].append(loss_value)

                # When outputs is None, do not calc metrics
                if not no_metrics:
                    # cache to cpu
                    outputs = outputs.detach().cpu()
                    labels = labels.detach().cpu()
                    outputss.append(outputs)
                    labelss.append(labels)

                # calc normal metrics
                batch_metrics_messages = []
                for k, metrics_fn in batch_metrics_fns.items():
                    v = metrics_fn(outputs, labels)
                    train_metrics[k].append(v)
                    batch_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

                # build message
                batch_message = f'loss:{self.get_loss_fmt().format(loss_value)} ' + \
                    ' '.join(batch_metrics_messages)

                t.set_description(f'{header}{batch_message}')
                t.refresh()

            epoch_loss = np.mean(train_metrics['loss'])

            # add loss manually
            train_history['loss'].append(epoch_loss)


            train_metrics_messages = []
            if not no_metrics:
                # gather values
                epoch_outputs = torch.cat(outputss)
                epoch_labels = torch.cat(labelss)

                for k, metrics_fn in epoch_metrics_fns.items():
                    vv = train_metrics.get(k)

                    # re-use metrics
                    if vv and k in self.get_mean_metrics():
                        v = np.mean(vv)
                    else:
                        v = metrics_fn(epoch_outputs, epoch_labels)
                    train_history[k].append(v)
                    train_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

            # build message
            train_message = f'loss:{self.get_loss_fmt().format(epoch_loss)} ' + ' '.join(train_metrics_messages)
            print(f'{header}train: {train_message}')

            #* validate
            val_metrics = {k:[] for k in batch_metrics_keys}
            if val_loader:
                self.model.eval()
                outputss = []
                labelss = []
                with torch.set_grad_enabled(False):
                    for i, (inputs, labels) in enumerate(val_loader):
                        loss, outputs = self.eval(inputs, labels, device)

                        loss_value = float(loss.item())
                        val_metrics['loss'].append(loss_value)

                        if not no_metrics:
                            # cache to cpu
                            outputs = outputs.detach().cpu()
                            labels = labels.detach().cpu()
                            outputss.append(outputs)
                            labelss.append(labels)
                            for k, metrics_fn in batch_metrics_fns.items():
                                val_metrics[k].append(metrics_fn(outputs, labels))

                mean_loss = np.mean(val_metrics['loss'])

                # add loss manually
                val_history['loss'].append(mean_loss)

                val_metrics_messages = []
                if not no_metrics:
                    epoch_outputs = torch.cat(outputss)
                    epoch_labels = torch.cat(labelss)

                    for k, metrics_fn in epoch_metrics_fns.items():
                        vv = val_metrics.get(k)

                        # re-use metrics
                        if vv and k in self.get_mean_metrics():
                            v = np.mean(vv)
                        else:
                            v = metrics_fn(epoch_outputs, epoch_labels)
                        val_history[k].append(v)
                        val_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

                val_message = f'loss:{self.get_loss_fmt().format(mean_loss)} ' + ' '.join(val_metrics_messages)
                print(f'{header}val: {val_message}')

            #* draw fig
            if epoch > 1:
                fisrt_idx = first_record_epoch
                x_axis = np.arange(1, epoch+1)[fisrt_idx:]
                count = len(train_history.keys())
                max_col = min(count, 3)
                max_row = math.ceil(count / max_col)
                # 1: 6, 2:8 ,3:10
                fig = plt.figure(figsize=(4+min(count, 3) * 3, 4 * max_row))
                for i, (k, train_values) in enumerate(train_history.items()):
                    ax = fig.add_subplot(max_row, max_col, i+1)
                    ax.get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))
                    ax.set_title(k)
                    ax.grid(axis='y')
                    ax.plot(x_axis, train_values[fisrt_idx:], label='train')
                    if val_loader:
                        val_values = val_history[k]
                        ax.plot(x_axis, val_values[fisrt_idx:], label='val')
                    ax.legend()
                fig_path = os.path.join(out_dir, 'curve.png')
                plt.savefig(fig_path)

                if epoch == 2 and show_fig:
                    subprocess.run(['/usr/bin/xdg-open', fig_path], check=True)
                fig.clf()
                plt.clf()
                plt.close()

            #* tensorboard
            for i, (k, train_values) in enumerate(train_history.items()):
                self.writer.add_scalar(f'{k}/train', train_values[-1], epoch)
                if val_loader:
                    val_values = val_history[k]
                    self.writer.add_scalar(f'{k}/val', val_values[-1], epoch)
            self.writer.flush()

            #* save weights
            if epoch % save_period == 0:
                weights_path = self.save_state(epoch, train_history, val_history)
                print(f'{header}Saved "{weights_path}"')

            self.step(scheduler, train_metrics['loss'][-1])
            print()

        writer.close()
        print('done.')
