import os
import re
import math
import random
import subprocess
from abc import ABCMeta, abstractmethod
from typing import NamedTuple
from collections import OrderedDict
from datetime import datetime

import numpy as np
import matplotlib
from matplotlib import ticker, pyplot as plt
import torch
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from tqdm import tqdm

from .torch import get_random_states, restore_random_states


class Checkpoint(NamedTuple):
    name: str
    suffix: str
    epoch: int
    model_state: OrderedDict
    optimizer_state: OrderedDict
    scheduler_state: OrderedDict
    train_history: dict
    val_history: dict
    random_states: dict

    def full_name(self):
        if self.suffix:
            return self.name + '_' + self.suffix
        return self.name


class Trainer(metaclass=ABCMeta):
    def __init__(self, name, model, loaders, device='cpu', save_period=10, show_fig=False, suffix=''):
        self.name = name
        self.model = model
        self.loaders = loaders
        self.device = device
        self.show_fig = show_fig
        self.save_period = save_period
        self.suffix = suffix

        self.current_epoch = -1

        self.metrics = {
            'batch': {},
            'epoch': {},
        }
        self.metrics.update(self.get_metrics())

        self.batch_metrics = self.metrics['batch']
        self.epoch_metrics = self.metrics['epoch']
        self.epoch_metrics.update(self.batch_metrics)
        self.no_metrics = len(self.batch_metrics) + len(self.epoch_metrics) == 0

        self.train_history = {k:[] for k in ['loss', *self.epoch_metrics.keys()]}
        self.val_history = {k:[] for k in ['loss', *self.epoch_metrics.keys()]}

        self.scheduler = None
        self.optimizer = None
        self.writer = None
        self.prepare()

    def with_suffix(self, s):
        if self.suffix:
            return s + '_' + self.suffix
        return s

    def prepare(self):
        pass

    def get_out_dir(self):
        return os.path.join('out', self.with_suffix(self.name))

    def get_model_state_dict(self):
        if isinstance(self.model, torch.nn.DataParallel):
            return self.model.module.state_dict()
        return self.model.state_dict()

    def create_optimizer(self, lr):
        return optim.RAdam(self.model.parameters(), lr=lr)

    def create_scheduler(self, lr):
        return optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=10)

    def get_loss_fmt(self):
        return '{:.4f}'

    def get_metrics(self):
        return { }

    def save_checkpoint(self, epoch):
        checkpoint = Checkpoint(
            name=self.name,
            suffix=self.suffix,
            epoch=epoch,
            model_state=self.get_model_state_dict(),
            optimizer_state=self.optimizer.state_dict(),
            scheduler_state=self.scheduler.state_dict(),
            train_history=self.train_history,
            val_history=self.val_history,
            random_states=get_random_states(),
        )
        chp_dir = os.path.join(self.get_out_dir(), 'checkpoints')
        os.makedirs(chp_dir, exist_ok=True)
        chp_path = os.path.join(chp_dir, f'{epoch}.pth')
        torch.save(checkpoint, chp_path)
        return chp_path

    def hook_load_state(self, checkpoint):
        pass

    def step(self, train_loss):
        if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            self.scheduler.step(train_loss)
            return
        self.scheduler.step()

    def require_writer(self):
        if not self.writer:
            self.writer = SummaryWriter(log_dir=f'./data/logs/{self.with_suffix(self.name)}')

    @abstractmethod
    def eval(self, inputs, labels):
        # return loss, outputs
        pass

    def start(self, max_epoch, lr=0.0001, checkpoint=None):
        print('Start training')
        if not self.show_fig:
            matplotlib.use('Agg')

        self.model.to(self.device)
        train_loader, val_loader = self.loaders
        self.optimizer = self.create_optimizer(lr)
        self.scheduler = self.create_scheduler(lr)

        out_dir = self.get_out_dir()
        os.makedirs(out_dir, exist_ok=True)

        initial_epoch = 1
        if checkpoint:
            self.model.load_state_dict(checkpoint.model_state)
            self.optimizer.load_state_dict(checkpoint.optimizer_state)
            restore_random_states(checkpoint.random_states)
            initial_epoch = checkpoint.epoch + 1
            data = (
                ('train', self.train_history, checkpoint.train_history),
                ('val', self.val_history, checkpoint.val_history)
            )
            self.require_writer()
            for target, history, chp_history in data:
                for k, vv in chp_history.items():
                    history[k] = vv
                    for i, v in enumerate(vv):
                        epoch = i + 1
                        self.writer.add_scalar(f'{k}/{target}', v, epoch)
            self.writer.flush()
            self.hook_load_state(checkpoint)

        for epoch in range(initial_epoch, max_epoch + 1):
            self.current_epoch = epoch
            header = f'[{epoch}/{max_epoch}] '
            # ReduceLROnPlateau
            lr = self.optimizer.param_groups[0]['lr']
            # LambdaLR
            # lr = scheduler.get_last_lr()[0]

            now = datetime.now().time().strftime('%X')
            print(f'{header}Starting lr={lr:.7f} ({now})')

            train_metrics_values = {k:[] for k in self.train_history.keys()}

            t = tqdm(train_loader, leave=False)
            outputss = []
            labelss = []

            #* train
            self.model.train()
            for (inputs, labels) in t:
                self.optimizer.zero_grad()
                loss, outputs = self.eval(inputs, labels)
                loss.backward()
                self.optimizer.step()
                loss_value = float(loss.item())

                # add loss to metrics table
                train_metrics_values['loss'].append(loss_value)

                # When outputs is None, do not calc metrics
                if not self.no_metrics:
                    # cache to cpu
                    outputs = outputs.detach().cpu()
                    labels = labels.detach().cpu()
                    outputss.append(outputs)
                    labelss.append(labels)

                # calc normal metrics
                batch_metrics_messages = []
                for k, metrics_fn in self.batch_metrics.items():
                    v = metrics_fn(outputs, labels)
                    train_metrics_values[k].append(v)
                    batch_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

                # build message
                batch_message = f'loss:{self.get_loss_fmt().format(loss_value)} ' + \
                    ' '.join(batch_metrics_messages)

                t.set_description(f'{header}{batch_message}')
                t.refresh()

            # collect metrics values
            train_epoch_loss = np.mean(train_metrics_values['loss'])
            # add loss manually
            self.train_history['loss'].append(train_epoch_loss)

            # unite other metrics values
            train_metrics_messages = []
            if not self.no_metrics:
                # gather values
                epoch_outputs = torch.cat(outputss)
                epoch_labels = torch.cat(labelss)

                for k, metrics_fn in self.epoch_metrics.items():
                    vv = train_metrics_values.get(k)

                    # re-use metrics
                    if vv and metrics_fn.use_mean:
                        v = np.mean(vv)
                    else:
                        v = metrics_fn(epoch_outputs, epoch_labels)
                    self.train_history[k].append(v)
                    train_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

            # build message
            train_message = f'loss:{self.get_loss_fmt().format(train_epoch_loss)} ' + ' '.join(train_metrics_messages)
            print(f'{header}train: {train_message}')

            #* validate
            val_metrics_values = {k:[] for k in self.val_history.keys()}
            if val_loader:
                self.model.eval()
                outputss = []
                labelss = []
                with torch.set_grad_enabled(False):
                    for i, (inputs, labels) in enumerate(val_loader):
                        loss, outputs = self.eval(inputs, labels)

                        loss_value = float(loss.item())
                        val_metrics_values['loss'].append(loss_value)

                        if not self.no_metrics:
                            # cache to cpu
                            outputs = outputs.detach().cpu()
                            labels = labels.detach().cpu()
                            outputss.append(outputs)
                            labelss.append(labels)
                            for k, metrics_fn in self.batch_metrics.items():
                                val_metrics_values[k].append(metrics_fn(outputs, labels))

                val_epoch_loss = np.mean(val_metrics_values['loss'])

                # add loss manually
                self.val_history['loss'].append(val_epoch_loss)

                val_metrics_messages = []
                if not self.no_metrics:
                    epoch_outputs = torch.cat(outputss)
                    epoch_labels = torch.cat(labelss)

                    for k, metrics_fn in self.epoch_metrics.items():
                        vv = val_metrics_values.get(k)

                        # re-use metrics
                        if vv and metrics_fn.use_mean:
                            v = np.mean(vv)
                        else:
                            v = metrics_fn(epoch_outputs, epoch_labels)
                        self.val_history[k].append(v)
                        val_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

                val_message = f'loss:{self.get_loss_fmt().format(val_epoch_loss)} ' + ' '.join(val_metrics_messages)
                print(f'{header}val: {val_message}')

            #* draw fig
            if epoch > 1:
                x_axis = np.arange(1, epoch+1)
                count = len(self.train_history.keys())
                max_col = min(count, 3)
                max_row = math.ceil(count / max_col)
                # 1: 6, 2:8 ,3:10
                fig = plt.figure(figsize=(4+min(count, 3) * 3, 4 * max_row))
                for i, (k, train_values) in enumerate(self.train_history.items()):
                    ax = fig.add_subplot(max_row, max_col, i+1)
                    ax.get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))
                    ax.set_title(k)
                    ax.grid(axis='y')
                    ax.plot(x_axis, train_values, label='train')
                    if val_loader:
                        val_values = self.val_history[k]
                        ax.plot(x_axis, val_values, label='val')
                    ax.legend()
                fig_path = os.path.join(out_dir, 'curve.png')
                plt.savefig(fig_path)

                if epoch == 1:
                    print(f'wrote {fig_path}')
                    if self.show_fig:
                        subprocess.run(['/usr/bin/xdg-open', fig_path], check=True)
                fig.clf()
                plt.clf()
                plt.close()

            #* tensorboard
            self.require_writer()

            groups = {
                None: ['loss'],
            }
            for k, metrics_fn in self.epoch_metrics.items():
                kk = groups.get(metrics_fn.group)
                if kk:
                    kk.append(k)
                else:
                    groups[metrics_fn.group] = [k]
            # groups = [
            #    None: ['loss', 'auc'],
            #    'acc': ['acc', 'recall', 'spec'],
            # ]

            historys = [['train', self.train_history]]
            if val_loader:
                historys.append(['val', self.val_history])

            for (target, history) in historys:
                for group_name, group in groups.items():
                    if group_name:
                        values = {}
                        for k in group:
                            values[k] = history[k][-1]
                        self.writer.add_scalars(f'{group_name}/{target}', values, epoch)
                    else:
                        for k in group:
                            self.writer.add_scalar(f'{k}/{target}', history[k][-1], epoch)

            self.writer.add_scalar('lr', lr, epoch)
            self.writer.flush()

            #* save checkpoints
            if epoch % self.save_period == 0:
                chp_path = self.save_checkpoint(epoch)
                print(f'{header}Saved "{chp_path}"')

            self.step(train_epoch_loss)
            print()

        self.writer.close()
        print('done.')
