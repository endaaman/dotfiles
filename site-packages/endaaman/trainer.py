import os
import re
import math
import random
import subprocess
from abc import ABCMeta, abstractmethod
from typing import NamedTuple
from collections import OrderedDict
from datetime import datetime

import numpy as np
import matplotlib
from matplotlib import ticker, pyplot as plt
import torch
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from tqdm import tqdm


class Checkpoint(NamedTuple):
    name: str
    epoch: int
    model_state: OrderedDict
    optimizer_state: OrderedDict
    train_history: dict
    val_history: dict


class Trainer(metaclass=ABCMeta):
    def __init__(self, name, model, loaders, device='cpu', save_period=10, show_fig=False):
        self.name = name
        self.model = model
        self.loaders = loaders
        self.device = device
        self.show_fig = show_fig
        self.save_period = 10

        self.current_epoch = -1

        self.batch_metrics = self.get_batch_metrics()
        self.epoch_metrics = self.get_epoch_metrics()
        self.epoch_metrics.update(self.batch_metrics)
        self.no_metrics = len(self.batch_metrics) + len(self.epoch_metrics) == 0

        self.train_history = {k:[] for k in ['loss', *self.epoch_metrics.keys()]}
        self.val_history = {k:[] for k in ['loss', *self.epoch_metrics.keys()]}

        self.writer = None
        self.prepare()

    def prepare(self):
        pass

    def get_metrics_to_mean(self):
        return []

    def get_out_dir(self):
        return os.path.join('out', self.name)

    def get_model_state_dict(self):
        if isinstance(self.model, torch.nn.DataParallel):
            return self.model.module.state_dict()
        return self.model.state_dict()

    def create_optimizer(self, lr):
        return optim.RAdam(self.model.parameters(), lr=lr)

    def create_scheduler(self, optimizer):
        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)

    def get_loss_fmt(self):
        return '{:.4f}'

    def get_batch_metrics(self):
        return {
            **self.get_epoch_metrics(),
        }

    def get_epoch_metrics(self):
        return {}

    def save_checkpoint(self, epoch, optimizer):
        checkpoint = Checkpoint(
            name=self.name,
            epoch=epoch,
            model_state=self.get_model_state_dict(),
            optimizer_state=optimizer.state_dict(),
            train_history=self.train_history,
            val_history=self.val_history,
        )
        chp_dir = os.path.join(self.get_out_dir(), 'checkpoints')
        os.makedirs(chp_dir, exist_ok=True)
        chp_path = os.path.join(chp_dir, f'{epoch}.pth')
        torch.save(checkpoint, chp_path)
        return chp_path

    def step(self, scheduler, epoch, last_loss):
        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(last_loss)
        else:
            scheduler.step()

    @abstractmethod
    def eval(self, inputs, labels):
        # return loss, outputs
        pass

    def start(self, max_epoch, lr=0.0001, checkpoint=None):
        print('Start training')
        if not self.show_fig:
            matplotlib.use('Agg')

        self.model.to(self.device)
        train_loader, val_loader = self.loaders
        optimizer = self.create_optimizer(lr)

        out_dir = self.get_out_dir()
        os.makedirs(out_dir, exist_ok=True)

        initial_epoch = 1
        if checkpoint:
            self.model.load_state_dict(checkpoint.model_state)
            optimizer.load_state_dict(checkpoint.optimizer)
            initial_epoch = checkpoint.epoch + 1
            data = (
                ('train', self.train_history, checkpoint.train_history),
                ('val', self.val_history, checkpoint.val_history)
            )
            for target, history, chp_history in data:
                for k, vv in chp_history:
                    history[k] = vv
                    for i, v in enumerate(vv):
                        epoch = i + 1
                        self.writer.add_scalar(f'{k}/{target}', v, epoch)
            self.writer.flush()

        scheduler = self.create_scheduler(optimizer)

        for epoch in range(initial_epoch, max_epoch + 1):
            self.current_epoch = epoch
            header = f'[{epoch}/{max_epoch}] '
            # ReduceLROnPlateau
            lr = optimizer.param_groups[0]['lr']
            # LambdaLR
            # lr = scheduler.get_last_lr()[0]

            now = datetime.now().time().strftime('%X')
            print(f'{header}Starting lr={lr:.7f} ({now})')

            train_metrics_values = {k:[] for k in self.train_history.keys()}

            t = tqdm(train_loader, leave=False)
            outputss = []
            labelss = []

            self.model.eval()
            for (inputs, labels) in t:
                optimizer.zero_grad()
                loss, outputs = self.eval(inputs, labels)
                loss.backward()
                optimizer.step()
                loss_value = float(loss.item())

                # add loss to metrics table
                train_metrics_values['loss'].append(loss_value)

                # When outputs is None, do not calc metrics
                if not self.no_metrics:
                    # cache to cpu
                    outputs = outputs.detach().cpu()
                    labels = labels.detach().cpu()
                    outputss.append(outputs)
                    labelss.append(labels)

                # calc normal metrics
                batch_metrics_messages = []
                for k, metrics_fn in self.batch_metrics.items():
                    v = metrics_fn(outputs, labels)
                    train_metrics_values[k].append(v)
                    batch_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

                # build message
                batch_message = f'loss:{self.get_loss_fmt().format(loss_value)} ' + \
                    ' '.join(batch_metrics_messages)

                t.set_description(f'{header}{batch_message}')
                t.refresh()

            # collect metrics values
            train_epoch_loss = np.mean(train_metrics_values['loss'])
            # add loss manually
            self.train_history['loss'].append(train_epoch_loss)

            # unite other metrics values
            train_metrics_messages = []
            if not self.no_metrics:
                # gather values
                epoch_outputs = torch.cat(outputss)
                epoch_labels = torch.cat(labelss)

                for k, metrics_fn in self.batch_metrics.items():
                    vv = train_metrics_values.get(k)

                    # re-use metrics
                    if vv and k in self.get_metrics_to_mean():
                        v = np.mean(vv)
                    else:
                        v = metrics_fn(epoch_outputs, epoch_labels)
                    self.train_history[k].append(v)
                    train_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

            # build message
            train_message = f'loss:{self.get_loss_fmt().format(train_epoch_loss)} ' + ' '.join(train_metrics_messages)
            print(f'{header}train: {train_message}')

            #* validate
            val_metrics_values = {k:[] for k in self.val_history.keys()}
            if val_loader:
                self.model.eval()
                outputss = []
                labelss = []
                with torch.set_grad_enabled(False):
                    for i, (inputs, labels) in enumerate(val_loader):
                        loss, outputs = self.eval(inputs, labels)

                        loss_value = float(loss.item())
                        val_metrics_values['loss'].append(loss_value)

                        if not no_metrics:
                            # cache to cpu
                            outputs = outputs.detach().cpu()
                            labels = labels.detach().cpu()
                            outputss.append(outputs)
                            labelss.append(labels)
                            for k, metrics_fn in self.batch_metrics.items():
                                val_metrics_values[k].append(metrics_fn(outputs, labels))

                val_epoch_loss = np.mean(val_metrics_values['loss'])

                # add loss manually
                val_history['loss'].append(val_epoch_loss)

                val_metrics_messages = []
                if not no_metrics:
                    epoch_outputs = torch.cat(outputss)
                    epoch_labels = torch.cat(labelss)

                    for k, metrics_fn in self.epoch_metrics.items():
                        vv = val_metrics_values.get(k)

                        # re-use metrics
                        if vv and k in self.get_metrics_to_mean():
                            v = np.mean(vv)
                        else:
                            v = metrics_fn(epoch_outputs, epoch_labels)
                        self.val_history[k].append(v)
                        val_metrics_messages.append(f'{k}:{metrics_fn.format(v)}')

                val_message = f'loss:{self.get_loss_fmt().format(val_epoch_loss)} ' + ' '.join(val_metrics_messages)
                print(f'{header}val: {val_message}')

            #* draw fig
            if epoch > 1:
                x_axis = np.arange(1, epoch+1)
                count = len(self.train_history.keys())
                max_col = min(count, 3)
                max_row = math.ceil(count / max_col)
                # 1: 6, 2:8 ,3:10
                fig = plt.figure(figsize=(4+min(count, 3) * 3, 4 * max_row))
                for i, (k, train_values) in enumerate(self.train_history.items()):
                    ax = fig.add_subplot(max_row, max_col, i+1)
                    ax.get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))
                    ax.set_title(k)
                    ax.grid(axis='y')
                    ax.plot(x_axis, train_values, label='train')
                    if val_loader:
                        val_values = self.val_history[k]
                        ax.plot(x_axis, val_values, label='val')
                    ax.legend()
                fig_path = os.path.join(out_dir, 'curve.png')
                plt.savefig(fig_path)

                if epoch == 1:
                    print(f'wrote {fig_path}')
                    if self.show_fig:
                        subprocess.run(['/usr/bin/xdg-open', fig_path], check=True)
                fig.clf()
                plt.clf()
                plt.close()

            #* tensorboard
            if not self.writer:
                self.writer = SummaryWriter(log_dir=f'./data/logs/{self.name}')
            for i, (k, train_values) in enumerate(self.train_history.items()):
                self.writer.add_scalar(f'{k}/train', train_values[-1], epoch)
                if val_loader:
                    val_values = self.val_history[k]
                    self.writer.add_scalar(f'{k}/val', val_values[-1], epoch)
            self.writer.add_scalar('lr', lr, epoch)
            self.writer.flush()

            #* save checkpoints
            if epoch % self.save_period == 0:
                chp_path = self.save_checkpoint(epoch, optimizer)
                print(f'{header}Saved "{chp_path}"')

            self.step(scheduler, train_epoch_loss)
            print()

        self.writer.close()
        print('done.')
