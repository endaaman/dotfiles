import os
import re
import math
import random
import subprocess
from abc import ABCMeta, abstractmethod
from typing import NamedTuple
from collections import OrderedDict
from datetime import datetime

import numpy as np
import matplotlib
from matplotlib import ticker, pyplot as plt
import torch
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import mlflow
from torchvision import transforms
from torchvision.utils import make_grid
from tqdm import tqdm

from .utils import wrap_iter
from .torch_utils import restore_random_states, get_random_states, smart_cat

DEFAULT_FORMAT = '.3f'

class Checkpoint(NamedTuple):
    model_name: str
    trainer_name: str
    epoch: int
    model_state: OrderedDict
    optimizer_state: OrderedDict
    scheduler_state: OrderedDict
    train_history: dict
    val_history: dict
    random_states: dict


class Trainer(metaclass=ABCMeta):
    def __init__(self,
                 model_name,
                 loaders,
                 experiment_name=None,
                 trainer_name=None,
                 log_dir='runs',
                 lr=0.01,
                 device='cpu',
                 save_period=-1,
                 extra=None):
        self.model_name = model_name
        self.loaders = loaders
        self.lr = lr
        self.trainer_name = trainer_name or model_name
        self.experiment_name = experiment_name
        self.log_dir = log_dir
        self.device = device
        self.save_period = save_period

        self.current_epoch = -1

        self.metrics = {
            'batch': {},
            'epoch': {},
        }

        self.scheduler = None
        self.optimizer = None
        self.writer = None

        self.extra = None
        # pylint: disable=assignment-from-no-return
        self.model = self.prepare(extra)

        self.metrics.update(self.get_metrics())
        self.batch_metrics = self.metrics['batch']
        self.epoch_metrics = self.metrics['epoch']
        self.epoch_metrics.update(self.batch_metrics)
        self.no_metrics = len(self.batch_metrics) + len(self.epoch_metrics) == 0
        self.train_history = {k:[] for k in ['loss', *self.epoch_metrics.keys()]}
        self.val_history = {k:[] for k in ['loss', *self.epoch_metrics.keys()]}

    @abstractmethod
    def prepare(self, extra):
        return None

    def get_out_dir(self):
        return os.path.join('out', self.experiment_name or 'default', self.trainer_name)

    def get_model_state_dict(self):
        if isinstance(self.model, torch.nn.DataParallel):
            return self.model.module.state_dict()
        return self.model.state_dict()

    def create_optimizer(self):
        return optim.RAdam(self.model.parameters(), lr=self.lr)

    def create_scheduler(self, total_epoch):
        # return optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=10)
        # return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda _: self.lr)
        return None

    def get_loss_fmt(self):
        return DEFAULT_FORMAT

    def get_metrics(self):
        return { }

    def save_checkpoint(self, epoch):
        checkpoint = Checkpoint(
            model_name=self.model_name,
            trainer_name=self.trainer_name,
            epoch=epoch,
            model_state=self.get_model_state_dict(),
            optimizer_state=self.optimizer.state_dict(),
            scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
            train_history=self.train_history,
            val_history=self.val_history,
            random_states=get_random_states(),
        )
        chp_dir = os.path.join(self.get_out_dir(), 'checkpoints')
        os.makedirs(chp_dir, exist_ok=True)
        chp_path = os.path.join(chp_dir, f'{epoch}.pth')
        torch.save(checkpoint, chp_path)
        return chp_path

    def hook_load_state(self, checkpoint):
        pass

    def step(self, train_loss):
        if self.scheduler is None:
            return
        if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            self.scheduler.step(train_loss)
            return
        # pylint: disable=no-value-for-parameter
        self.scheduler.step()

    def prepare_mlops(self):
        if not self.writer:
            base_tb_log_dir = os.path.join(*list(filter(None, [
                self.log_dir,
                'tensorboard',
                self.experiment_name or 'default',
                self.trainer_name
            ])))
            idx = 0
            ok = False
            tb_log_dir = base_tb_log_dir
            while not ok:
                tb_log_dir = base_tb_log_dir + f'_{idx}'
                if not os.path.exists(tb_log_dir):
                    ok = True
                idx += 1
            self.writer = SummaryWriter(log_dir=tb_log_dir)
            print(f'using log dir: tensorboard=`{tb_log_dir}`')

        # mlflow ui --backend-store-uri ./runs/mlflow
        if mlflow.active_run() is None:
            mf_log_dir = os.path.join(self.log_dir, 'mlflow')
            mlflow.set_tracking_uri(mf_log_dir)
            name = self.experiment_name or 'Default'
            experiment = mlflow.get_experiment_by_name(name=name)
            if experiment:
                experiment_id = experiment.experiment_id
            else:
                experiment_id = mlflow.create_experiment(name=name)
            mlflow.start_run(experiment_id=experiment_id, run_name=self.trainer_name)
            for k, v in self.additional_args.items():
                mlflow.log_param(k, v)
            print(f'using log dir: mlflow=`{mf_log_dir}`')

    @abstractmethod
    def eval(self, inputs, gts):
        # 以下は外で
        # self.optimizer.zero_grad()
        # loss.backward()
        # return loss, preds
        pass

    def eval_image(self, inputs, gts, preds):
        # return images
        pass

    def merge_images(self, images):
        return None

    def record_value(self, k, value, step):
        self.writer.add_scalar(k, value, step)
        mlflow.log_metric(k, value, step=step)

    def start(self, total_epoch, checkpoint=None):
        print('Start training')
        matplotlib.use('Agg')

        self.model.to(self.device)
        train_loader, val_loader = self.loaders
        self.optimizer = self.create_optimizer()
        self.scheduler = self.create_scheduler(total_epoch)

        out_dir = self.get_out_dir()
        os.makedirs(out_dir, exist_ok=True)

        initial_epoch = 1
        if checkpoint:
            self.model.load_state_dict(checkpoint.model_state)
            self.optimizer.load_state_dict(checkpoint.optimizer_state)
            restore_random_states(checkpoint.random_states)
            initial_epoch = checkpoint.epoch + 1
            data = (
                ('train', self.train_history, checkpoint.train_history),
                ('val', self.val_history, checkpoint.val_history)
            )
            self.prepare_mlops()
            for target, history, chp_history in data:
                for k, vv in chp_history.items():
                    history[k] = vv
                    for i, v in enumerate(vv):
                        epoch = i + 1
                        self.writer.add_scalar(f'{k}/{target}', v, epoch)
            self.writer.flush()
            self.hook_load_state(checkpoint)

        for epoch in range(initial_epoch, total_epoch + 1):
            self.current_epoch = epoch
            header = f'[{epoch}/{total_epoch}] '
            # ReduceLROnPlateau
            current_lr = self.optimizer.param_groups[0]['lr']
            # LambdaLR
            # lr = scheduler.get_last_lr()[0]

            now = datetime.now().time().strftime('%X')
            print(f'{header}Starting lr={current_lr:.7f} ({now})')

            train_metrics_values = {k:[] for k in self.train_history.keys()}

            t = tqdm(train_loader, leave=False)
            predss = []
            gtss = []

            #* train
            self.model.train()
            for (inputs, gts) in t:
                self.optimizer.zero_grad()
                loss, preds = self.eval(inputs, gts)
                loss.backward()
                self.optimizer.step()
                loss_value = float(loss.item())

                # add loss to metrics table
                train_metrics_values['loss'].append(loss_value)

                # TODO: zipなどできるように
                preds = preds.detach().cpu()
                gts = gts.detach().cpu()

                # When preds is None, do not calc metrics
                if not self.no_metrics:
                    # cache to cpu
                    predss.append(preds)
                    gtss.append(gts)

                # calc normal metrics
                batch_metrics_messages = []
                for k, metrics_fn in self.batch_metrics.items():
                    v = metrics_fn(preds, gts)
                    train_metrics_values[k].append(v)
                    fmt = getattr(metrics_fn, 'format', DEFAULT_FORMAT)
                    batch_metrics_messages.append(f'{k}:{v:{fmt}}')

                # build message
                batch_message = f'loss:{loss_value:{self.get_loss_fmt()}} ' + \
                    ' '.join(batch_metrics_messages)

                t.set_description(f'{header}{batch_message}')
                t.refresh()

            # collect metrics values
            train_epoch_loss = np.mean(train_metrics_values['loss'])
            # add loss manually
            self.train_history['loss'].append(train_epoch_loss)

            # unite other metrics values
            train_metrics_messages = []
            if not self.no_metrics:
                # gather values
                epoch_preds = smart_cat(predss)
                epoch_gts = smart_cat(gtss)

                for k, metrics_fn in self.epoch_metrics.items():
                    vv = train_metrics_values.get(k)

                    # re-use metrics
                    if vv and metrics_fn.use_mean:
                        v = np.mean(vv)
                    else:
                        v = metrics_fn(epoch_preds, epoch_gts)
                    self.train_history[k].append(v)
                    fmt = getattr(metrics_fn, 'format', DEFAULT_FORMAT)
                    train_metrics_messages.append(f'{k}:{v:{fmt}}')

            # build message
            train_message = f'loss:{train_epoch_loss:{self.get_loss_fmt()}} ' + ' '.join(train_metrics_messages)
            print(f'{header}train: {train_message}')

            #* validate
            val_metrics_values = {k:[] for k in self.val_history.keys()}
            if val_loader:
                self.model.eval()
                predss = []
                gtss = []
                # images = []
                with torch.set_grad_enabled(False):
                    t = tqdm(enumerate(val_loader), leave=False, total=len(val_loader))
                    t.set_description(f'{header}Evaluating validation dataset..')
                    for i, (inputs, gts) in t:
                        loss, preds = self.eval(inputs, gts)

                        # if self.report_image:
                        #     images += self.eval_image(inputs, gts, preds)

                        loss_value = float(loss.item())
                        val_metrics_values['loss'].append(loss_value)

                        if not self.no_metrics:
                            # cache to cpu
                            preds = preds.detach().cpu()
                            gts = gts.detach().cpu()
                            predss.append(preds)
                            gtss.append(gts)
                            for k, metrics_fn in self.batch_metrics.items():
                                val_metrics_values[k].append(metrics_fn(preds, gts))

                val_epoch_loss = np.mean(val_metrics_values['loss'])

                # add loss manually
                self.val_history['loss'].append(val_epoch_loss)

                val_metrics_messages = []
                if not self.no_metrics:
                    epoch_preds = smart_cat(predss)
                    epoch_gts = smart_cat(gtss)

                    for k, metrics_fn in self.epoch_metrics.items():
                        vv = val_metrics_values.get(k)
                        # re-use metrics
                        if vv and metrics_fn.use_mean:
                            v = np.mean(vv)
                        else:
                            v = metrics_fn(epoch_preds, epoch_gts)
                        self.val_history[k].append(v)
                        fmt = getattr(metrics_fn, 'format', DEFAULT_FORMAT)
                        val_metrics_messages.append(f'{k}:{v:{fmt}}')

                val_message = f'loss:{val_epoch_loss:{self.get_loss_fmt()}} ' + ' '.join(val_metrics_messages)
                print(f'{header}val: {val_message}')


            #* draw fig
            if epoch > 1:
                x_axis = np.arange(1, epoch+1)
                count = len(self.train_history.keys())
                max_col = min(count, 3)
                max_row = math.ceil(count / max_col)
                # 1: 6, 2:8 ,3:10
                fig = plt.figure(figsize=(4+min(count, 3) * 3, 4 * max_row))
                for i, (k, train_values) in enumerate(self.train_history.items()):
                    ax = fig.add_subplot(max_row, max_col, i+1)
                    ax.get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))
                    ax.set_title(k)
                    ax.grid(axis='y')
                    ax.plot(x_axis, train_values, label='train')
                    if val_loader:
                        val_values = self.val_history[k]
                        ax.plot(x_axis, val_values, label='val')
                    ax.legend()
                fig_path = os.path.join(out_dir, 'curve.png')
                plt.savefig(fig_path)

                if epoch == 1:
                    print(f'wrote {fig_path}')
                fig.clf()
                plt.clf()
                plt.close()

            #* tensorboard
            self.prepare_mlops()

            groups = {
                None: ['loss'],
            }
            for k, metrics_fn in self.epoch_metrics.items():
                kk = groups.get(metrics_fn.group)
                if kk:
                    kk.append(k)
                else:
                    groups[metrics_fn.group] = [k]
            # groups = [
            #    None: ['loss', 'auc'],
            #    'acc': ['acc', 'recall', 'spec'],
            # ]

            historys = [['train', self.train_history]]
            if val_loader:
                historys.append(['val', self.val_history])

            for (target, history) in historys:
                for group_name, group in groups.items():
                    if group_name:
                        values = {}
                        for k in group:
                            values[k] = history[k][-1]
                        self.writer.add_scalars(f'{group_name}/{target}', values, epoch)
                    else:
                        for k in group:
                            self.record_value(f'{k}/{target}', history[k][-1], epoch)

            self.record_value('lr', current_lr, epoch)
            # if self.report_image:
            #     pass
            #     image = self.merge_images(images)
            #     if image is not None and image.any():
            #         self.writer.add_image('image/val', image, epoch)

            self.writer.flush()

            #* save checkpoints
            if (self.save_period > 0 and epoch % self.save_period == 0) or (self.save_period <= 0 and epoch == total_epoch):
                chp_path = self.save_checkpoint(epoch)
                print(f'{header}Saved "{chp_path}"')

            self.step(train_epoch_loss)
            print()

        self.writer.close()
        mlflow.end_run()
        print('done.')
