
import torch
import sys



def compute_joint(x, y):
    p_x_y = x.unsqueeze(2) * y.unsqueeze(1)
    print('mul')
    print(p_x_y)

    print('sum')
    p_x_y = p_x_y.sum(dim=0)
    print(p_x_y)

    p_x_y = (p_x_y + p_x_y.t()) / 2.

    p_x_y = p_x_y / p_x_y.sum()
    return p_x_y


def iic_loss(x, y):
    EPS = sys.float_info.epsilon

    bs, k = x.size()
    p_x_y = compute_joint(x, y)

    p_x = p_x_y.sum(dim=1).view(k, 1)
    print('p_x')
    print(p_x)
    p_x = p_x.expand(k, k)
    print('p_x expand')
    print(p_x)
    print()

    # p_y = p_x_y.sum(dim=0).view(1, k)
    # print('p_y')
    # print(p_y)
    # p_y = p_y.expand(k, k)
    # print('p_y')
    # print(p_y)
    p_y = p_x.t()

    p_x_y = torch.clamp(p_x_y, min=EPS)
    p_x = torch.clamp(p_x, min=EPS)
    p_y = torch.clamp(p_y, min=EPS)

    alpha = 2.0  # 論文や通常の相互情報量の計算はalphaは1です


    print('log xy')
    print(torch.log(p_x_y))
    print('log x')
    print(torch.log(p_x))
    print('log y')
    print(torch.log(p_y))
    print()

    loss_xy = p_x_y * torch.log(p_x_y)
    loss_x = p_x_y * torch.log(p_x)
    loss_y = p_x_y * torch.log(p_y)

    print('loss xy')
    print(loss_xy)
    print('loss x')
    print(loss_x)
    print('loss y')
    print(loss_y)
    print()

    loss = loss_xy - loss_x - loss_y

    loss = loss.sum()

    return loss


x = torch.tensor([
    [0.8, 0.1, 0.05, 0.05],
    [0.1, 0.7, 0.1, 0.1],
])
y = torch.tensor([
    [0.1, 0.8, 0.1, 0.0],
    [0.1, 0.3, 0.2, 0.1],
])

P = compute_joint(x, y)
print(P)

loss = iic_loss(x, y)
print(loss)


exit(0)

def same(a, b):
    return torch.sum(a*torch.log(b) + b*torch.log(a)).mean()

def diff(a, b):
    return torch.sum((torch.log(a) * torch.log(b)) / torch.log(a * b))

print('same')
print(same(x, y))
print('diff')
print(diff(x, y))


y = torch.tensor([0.7, 0.1, 0.1, 0.1])

print('same')
print(same(x, y))
print('diff')
print(diff(x, y))
