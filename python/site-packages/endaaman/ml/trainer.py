import os
import re
import math
import sys
import random
import subprocess
import json
import shutil
import inspect
from abc import ABCMeta, abstractmethod
from typing import NamedTuple
from collections import OrderedDict
from datetime import datetime
import hashlib

from PIL import Image
from tqdm import tqdm
import pydantic
from pydantic import BaseModel
import numpy as np
import pandas as pd
import matplotlib
from matplotlib import ticker, pyplot as plt
import torch
from torch import optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
# from tensorboardX import SummaryWriter
from torchvision import transforms
from torchvision.utils import make_grid
import mlflow

from ..utils import wrap_iter, yes_no_input
from .metrics import BaseMetrics
from .utils import *

__all__ = [
    'Checkpoint',
    'BaseTrainer',
    'BaseTrainerConfig',
]


J = os.path.join

CONFIG_FILE = 'config.json'
PLOT_FILE = 'plot.png'
LAST_CHECKPOINT_FILE = 'checkpoint_last.pt'
BEST_CHECKPOINT_FILE = 'checkpoint_best.pt'
METRICS_FILE = 'metrics.xlsx'
LAST_PREDICTIONS_FILE = 'predictions_last.pt'
BEST_PREDICTIONS_FILE = 'predictions_best.pt'

DEFAULT_FORMAT = '{:.3f}'


class BaseTrainerConfig(BaseModel):
    seed: int = get_global_seed()
    num_workers:int = 4
    batch_size:int
    lr:float
    model_hash: str = ''
    class Config:
        extra = pydantic.Extra.forbid
        validate_assignment = True
        # frozen = True

    @classmethod
    def from_file(cls, config_path):
        with open(config_path, mode='r', encoding='utf-8') as f:
            d = json.load(f)
            config = cls(**d)
        return config

    def to_json(self, json_path):
        assert self.model_hash
        with open(json_path, 'w', encoding='utf-8') as f:
            f.write(self.json(indent=2))

class Checkpoint(NamedTuple):
    config: dict
    epoch: int
    model_state: dict
    optimizer_state: dict
    scheduler_state: dict
    train_history: dict
    val_history: dict
    random_states: dict


class EvacuateModel:
    def __init__(self, model, device):
        self.model = model
        self.device = device

    def __enter__(self):
        self.model.cpu()
        return self.model

    def __exit__(self, *args):
        self.model.to(self.device)

def as_primitive(v):
    if isinstance(v, (list, tuple)):
        return torch.tensor(v)
    return v

class BaseTrainer(metaclass=ABCMeta):
    def __init__(self,
                 config:BaseTrainerConfig,
                 out_dir:str,
                 train_dataset:Dataset = None,
                 val_dataset:Dataset = None,
                 use_gpu = True,
                 experiment_name = 'Default',
                 loss_fmt = DEFAULT_FORMAT,
                 main_metrics = '-loss',
                 overwrite = False,
                 fig_size = None,
                 fig_col_count = 3,
                 ):
        self.config = config
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.experiment_name = experiment_name
        self.use_gpu = use_gpu
        self.loss_fmt = loss_fmt
        self.main_metrics = main_metrics
        self.overwrite = overwrite
        self.fig_size = fig_size
        self.fig_col_count = fig_col_count

        self.scheduler = None
        self.optimizer = None
        self.writer = None
        self.train_loader = None
        self.val_loader = None
        self.device = torch.device('cuda' if self.use_gpu and torch.cuda.is_available() else 'cpu')
        self.model = self.prepare()

        self.optimizer = self.create_optimizer()
        self.scheduler = self.create_scheduler()

        self.metrics:dict[str, BaseMetrics] = {}
        self.metrics.update(self.get_metrics())
        self.visualizers = {}
        self.visualizers.update(self.get_visualizers())
        self.hooks = {}
        self.hooks.update(self.get_hooks())

        for name in dir(self):
            m = re.match('^metrics_(.*)$', name)
            if m:
                self.metrics[m[1]] = getattr(self, name)
            m = re.match('^visualize_(.*)$', name)
            if m:
                self.visualizers[m[1]] = getattr(self, name)

            m = re.match('^hook_(.*)$', name)
            if m:
                self.hooks[m[1]] = getattr(self, name)

        keys = ['loss']
        self.formats = {'loss': self.loss_fmt}
        for k, m in self.metrics.items():
            keys += k.split('_')
            for key in keys:
                self.formats[key] = getattr(m, 'format', DEFAULT_FORMAT)
        self.train_history = {k:[] for k in keys}
        self.val_history = {k:[] for k in keys}

        for (ds, is_train) in ((train_dataset, True), (val_dataset, False)):
            l = DataLoader(
                dataset=ds,
                batch_size=config.batch_size,
                num_workers=config.num_workers,
                shuffle=is_train,
            ) if ds else None
            if is_train:
                self.train_loader = l
            else:
                self.val_loader = l

        # state
        self.current_epoch = 1
        self.best_metric = -1
        self.checkpoint = None

        # write config before run
        text = str(self.model).encode()
        model_hash = hashlib.sha256(text).hexdigest()[:10]
        config.model_hash = model_hash

        # select out dir
        self.out_dir, chp_path = self.select_out_dir(out_dir)
        if chp_path:
            print(f'Automatically restoring from {chp_path}')
            checkpoint = torch.load(chp_path)
            if isinstance(checkpoint, Checkpoint):
                pass
            elif isinstance(checkpoint, dict):
                checkpoint = Checkpoint(**checkpoint)
            else:
                raise RuntimeError('Checkpoint is invalid type:', type(checkpoint))
            self.restore(checkpoint=checkpoint)

        config_file_path = J(self.out_dir, CONFIG_FILE)
        config.to_json(config_file_path)
        print(f'wrote {config_file_path}')


    def select_out_dir(self, out_dir):
        # returns needs to restore checkpoint
        if not os.path.isdir(out_dir):
            os.makedirs(out_dir)
            return out_dir, None

        if self.overwrite:
            # reset old out
            shutil.rmtree(out_dir)
            os.makedirs(out_dir)
            return out_dir, None

        i = 0
        result_out_dir = out_dir
        result_chp_path = None
        while True:
            if not os.path.exists(result_out_dir):
                print(f'New out dir: {result_out_dir}')
                break

            checkpoint_file_path = J(result_out_dir, LAST_CHECKPOINT_FILE)
            if not os.path.exists(checkpoint_file_path):
                print(f'Out dir `{result_out_dir}` exists but, theres is no checkpoint file.')
                break

            config_file_path = J(result_out_dir, CONFIG_FILE)
            with open(config_file_path, 'r', encoding='utf-8') as f:
                old_config = json.load(f)

            if old_config == self.config.dict():
                chp_path = J(result_out_dir, LAST_CHECKPOINT_FILE)
                if os.path.exists(chp_path):
                    result_chp_path = chp_path
                else:
                    print(f'Old checkpoint {chp_path} does not exist. Starting from the begining.')
                break
            print(f'The old config {config_file_path} does not match to current.')
            # print(f'The old config {config_file_path} does not match to current one \n' \
            #       f'[current]:\n{self.config}\n' \
            #       f'[old] :\n{old_config}\n')

            result_out_dir = f'{out_dir}_{i}'
            i += 1

        os.makedirs(result_out_dir, exist_ok=True)
        return result_out_dir, result_chp_path


    @abstractmethod
    def prepare(self):
        # create model
        return None

    def get_model_state_dict(self):
        if isinstance(self.model, torch.nn.DataParallel):
            return self.model.module.state_dict()
        return self.model.state_dict()

    def create_optimizer(self):
        return optim.RAdam(self.model.parameters(), lr=self.config.lr)

    def create_scheduler(self):
        # return optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=10)
        # return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda _: self.lr)
        return None

    def is_achieved_best(self):
        name = self.main_metrics
        if name[0] == '-':
            name = name[1:]
            func = np.argmin
        else:
            func = np.argmax
        h = self.val_history if self.val_loader else self.train_history
        return func(h[name]) == len(h[name]) - 1

    def continues(self):
        # if True: continue
        return True

    def get_metrics(self):
        return { }

    def get_visualizers(self):
        return { }

    def get_hooks(self):
        return { }

    def save_metrics(self, path):
        hh = {
            'train': self.train_history,
            'val': self.val_history,
        }
        with pd.ExcelWriter(path, engine='xlsxwriter') as writer:
            for t, history in hh.items():
                if not history:
                    continue
                df = pd.DataFrame(history, index=np.arange(len(history['loss']))+1)
                df.index.name = 'epoch'
                df.to_excel(writer, sheet_name=t)
                worksheet = writer.sheets[t]
                for i, k in enumerate(history.keys()):
                    base = self.formats[k].format(.0)
                    num_format = writer.book.add_format({'num_format': f'#,##{base}'})
                    worksheet.set_column(i+1, i+1, None, num_format)

    def save_checkpoint(self, path):
        with EvacuateModel(self.model, self.device):
            checkpoint = Checkpoint(
                config=self.config.dict(),
                epoch=self.current_epoch,
                model_state=self.get_model_state_dict(),
                optimizer_state=self.optimizer.state_dict(),
                scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
                train_history=self.train_history,
                val_history=self.val_history,
                random_states=get_random_states(),
            )
        torch.save(checkpoint._asdict(), path)

    def step(self):
        if self.scheduler is None:
            return
        if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            # last_loss = self.train_history['loss'][-1]
            history = self.val_history or self.train_history
            self.scheduler.step(history['loss'][-1])
            return
        # pylint: disable=no-value-for-parameter
        self.scheduler.step()

    def prepare_mlops(self):
        if not self.writer:
            tb_log_dir = J(self.out_dir, 'logs')
            is_first_time = not os.path.isdir(tb_log_dir)
            self.writer = SummaryWriter(log_dir=tb_log_dir)
            print(f'using log dir: tensorboard=`{tb_log_dir}`')
            # if is_first_time:
            #     hparams = {k:as_primitive(v) for k, v in self.config.dict().items()}
            #     print('add hpaparams', hparams)
            #     self.writer.add_hparams(hparams, {}, run_name='.')

        # mlflow ui --backend-store-uri ./runs/mlflow
        if mlflow.active_run() is None:
            # mlflow.set_tracking_uri('mlruns')
            experiment = mlflow.get_experiment_by_name(name=self.experiment_name)
            if experiment:
                experiment_id = experiment.experiment_id
            else:
                experiment_id = mlflow.create_experiment(name=self.experiment_name)

            s = mlflow.search_runs()
            run_id = None
            if len(s) > 0:
                r = s[(s['experiment_id'] == experiment_id) & (s['tags.mlflow.runName'] == self.out_dir)]
                if len(r) > 1:
                    print('Multiple runs exist. Resuming last run.')
                if len(r) > 0:
                    run_id = r.iloc[0]['run_id']
            if run_id:
                if not self.overwrite:
                    mlflow.start_run(experiment_id=experiment_id, run_id=run_id)
                    return
                mlflow.delete_run(run_id=run_id)
            mlflow.start_run(experiment_id=experiment_id, run_name=self.out_dir)
            for k, v in self.config.dict().items():
                mlflow.log_param(k, v)

    def _eval(self, inputs, gts, is_training):
        self.model.to(self.device)
        if is_training:
            self.optimizer.zero_grad()
        with torch.set_grad_enabled(is_training):
            loss, preds = self.eval(inputs, gts)
        if is_training:
            loss.backward()
            self.optimizer.step()
        return loss, preds

    @abstractmethod
    def eval(self, inputs, gts):
        # DO:
        # - return loss, preds
        # DO NOT:
        # - model.to(self.device)
        # - self.optimizer.zero_grad()
        # - loss.backward()
        pass

    def record_value(self, k, value, step):
        self.writer.add_scalar(k, value, step)
        mlflow.log_metric(k, value, step)

    def restore(self, checkpoint: Checkpoint):
        with EvacuateModel(self.model, self.device):
            self.model.load_state_dict(checkpoint.model_state)
        self.optimizer.load_state_dict(checkpoint.optimizer_state)
        if self.scheduler:
            self.scheduler.load_state_dict(checkpoint.scheduler_state)
        restore_random_states(checkpoint.random_states)
        data = (
            ('train', self.train_history, checkpoint.train_history),
            ('val', self.val_history, checkpoint.val_history)
        )
        self.prepare_mlops()
        for target, history, chp_history in data:
            for k, vv in chp_history.items():
                history[k] = vv
                # for i, v in enumerate(vv):
                #     epoch = i + 1
                #     self.record_value(f'{k}/{target}', v, epoch)
        self.writer.flush()
        self.current_epoch = checkpoint.epoch + 1
        self.checkpoint = checkpoint


    def calc_metrics_and_get_message(self, preds, gts, history=None):
        messages = []
        for key, m in self.metrics.items():
            vv = m(preds, gts)
            if vv is None:
                continue
            vv = vv if isinstance(vv, (list, tuple)) else (vv, )
            kk = key.split('_')
            if len(kk) != len(vv):
                raise RuntimeError('kk and vv are defferent len: kk=', len(kk), 'vv=', len(vv))
            for k, v in zip(kk, vv):
                if history:
                    history[k].append(v)
                v_str = self.formats[k].format(v)
                messages.append(f'{k}:{v_str}')
        return ' '.join(messages)


    def evaluate_loader(self, is_training, loader, history):
        if is_training:
            target = 'train'
            self.model.train()
        else:
            target = 'val'
            self.model.eval()

        losses = []
        t = tqdm(loader, leave=False, dynamic_ncols=True)
        predss = []
        gtss = []
        for (inputs, gts) in t:
            loss, preds, = self._eval(inputs, gts, is_training)
            loss_value = float(loss.item())
            losses.append(loss_value)
            batch_metrics_message = self.calc_metrics_and_get_message(preds, gts)
            loss_str = self.loss_fmt.format(loss_value)
            t.set_description(f'[{target}] loss:{loss_str} ' + batch_metrics_message)
            t.refresh()
            predss.append(preds)
            gtss.append(gts)

        epoch_loss = np.mean(losses)
        history['loss'].append(epoch_loss)

        # gather values
        epoch_preds = smart_cat(predss)
        epoch_gts = smart_cat(gtss)

        epoch_metrics_message = self.calc_metrics_and_get_message(epoch_preds, epoch_gts, history)

        # build message
        loss_str = self.loss_fmt.format(epoch_loss)
        print(f'[{target}]: loss:{loss_str} ' + epoch_metrics_message)

        return epoch_preds, epoch_gts

    def plot(self, train_preds, train_gts, val_preds, val_gts):
        mp_backend = matplotlib.rcParams['backend']
        matplotlib.use('Agg')

        x_axis = np.arange(1, self.current_epoch+1)
        metrics_count = len(self.train_history)
        count = metrics_count + len(self.visualizers)
        col_count = min(count, self.fig_col_count)
        row_count = math.ceil(count / col_count)
        figsize = self.fig_size or (4+min(count, 3) * 3, 4 * row_count) # 1:6, 2:8, 3:10
        fig, axes = plt.subplots(row_count, col_count, figsize=figsize)
        for i, (k, train_values) in enumerate(self.train_history.items()):
            y = i // col_count
            x = i % col_count
            if row_count > 1:
                ax = axes[y, x]
            elif col_count > 1:
                ax = axes[x]
            else:
                ax = axes
            ax.get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))
            ax.set_title(k)
            ax.grid(axis='y')
            ax.plot(x_axis, train_values, label='train')
            if self.val_loader:
                val_values = self.val_history[k]
                ax.plot(x_axis, val_values, label='val')
            ax.legend()

        i = metrics_count
        for k, vis in self.visualizers.items():
            y = i // col_count
            x = i % col_count
            ax = axes[y, x] if row_count > 1 else axes[x]
            args = [ax, train_preds, train_gts, val_preds, val_gts]
            if not inspect.ismethod(vis):
                args.insert(0, self)
            vis(*args)
            i += 1

        fig_path = os.path.join(self.out_dir, PLOT_FILE)
        plt.savefig(fig_path)

        if self.current_epoch == 1:
            print(f'wrote {fig_path}')
        fig.clf()
        plt.clf()
        plt.close()
        matplotlib.use(mp_backend)
        img = Image.open(fig_path)
        self.writer.add_image('plot', pil_to_tensor(img), self.current_epoch)
        # TODO: log artifact
        # mlflow.log_image(img, 'plot.png')

    def save_predictions(self, train_preds, train_gts, val_preds, val_gts):
        vv = (train_preds, train_gts, val_preds, val_gts)
        if all(v is None for v in vv):
            return
        names = ('train_preds', 'train_gts', 'val_preds', 'val_gts')
        # vv = [None if v is None else v.detach().cpu() for v in vv]
        d = dict(zip(names, vv))
        torch.save(d, J(self.out_dir, LAST_PREDICTIONS_FILE))
        if not self.is_achieved_best():
            return
        torch.save(d, J(self.out_dir, BEST_PREDICTIONS_FILE))

    def get_current_lr(self):
        # LambdaLR
        # return scheduler.get_last_lr()[0]
        # ReduceLROnPlateau
        return self.optimizer.param_groups[0]['lr']

    def start(self, total_epoch):
        if not self.train_loader:
            raise RuntimeError('train loader is not provided: ', self.train_loader)
        print(f'Start training [{self.current_epoch}/{total_epoch}]')
        print(f'config: {self.config}')
        self.model.to(self.device)

        while self.current_epoch <= total_epoch:
            now = datetime.now().time().strftime('%X')
            print(f'[{self.current_epoch}/{total_epoch}] Start training ({now})')

            train_preds, train_gts = self.evaluate_loader(True, self.train_loader, self.train_history)

            if self.val_loader:
                with torch.set_grad_enabled(False):
                    val_preds, val_gts = self.evaluate_loader(False, self.val_loader, self.val_history)
            else:
                val_preds = None
                val_gts = None

            #* draw fig
            if self.current_epoch > 1:
                self.plot(train_preds, train_gts, val_preds, val_gts)
            self.save_predictions(train_preds, train_gts, val_preds, val_gts)

            for k, hook in self.hooks.items():
                args = [train_preds, train_gts, val_preds, val_gts]
                if not inspect.ismethod(hook):
                    args.insert(0, self)
                hook(*args)

            self.prepare_mlops()
            values_with_hparams = {}
            for (t, history) in (('train', self.train_history), ('val', self.val_history)):
                for k, values in history.items():
                    if len(values) > 0:
                        tag = f'{k}/{t}'
                        self.record_value(tag, values[-1], self.current_epoch)
                        values_with_hparams[f'hparam/{tag}'] = values[-1]
            self.record_value('lr', self.get_current_lr(), self.current_epoch)
            hparams = {k:as_primitive(v) for k, v in self.config.dict().items()}
            self.writer.add_hparams(hparams, values_with_hparams, run_name='.')
            self.writer.flush()

            self.save_checkpoint(J(self.out_dir, LAST_CHECKPOINT_FILE))
            self.save_metrics(J(self.out_dir, METRICS_FILE))
            if self.is_achieved_best():
                self.save_checkpoint(J(self.out_dir, BEST_CHECKPOINT_FILE))
                # for h, t in ((self.train_history, 'train'), (self.val_history, 'val')):
                #     if not h:
                #         continue
                #     for k, v in h.items():
                #         if len(v) > 0:
                #             mlflow.log_metric(f'best/{k}', v[-1], len(v))
                print('save best checkpoint')

            self.step()
            self.current_epoch += 1
            print()
            continues = self.continues()
            if not continues:
                print('Reached to the early stopping condition.')
                break

        self.writer.close()
        mlflow.end_run()
        print('done.')
