import os
import re
import math
import random
from pathlib import Path
import subprocess
from typing import TypeVar
from functools import lru_cache

import torch
from pydantic import BaseModel, Field

# from .predictor import BasePredictor
from ..cli import BaseCLI, field
# pylint: disable=unused-wildcard-import,wildcard-import
from .utils import *


__all__ = [
    'field',
    'BaseMLCLI',
    'BaseMLArgs',
    'BaseDLArgs',
]


torch.multiprocessing.set_sharing_strategy('file_system')



class BaseMLArgs(BaseModel):
    seed: int = get_global_seed()

class BaseDLArgs(BaseMLArgs):
    cpu: bool = False

    def device(self):
        return torch.device('cuda' if torch.cuda.is_available() and not self.cpu else 'cpu')

class BaseTrainArgs(BaseDLArgs):
    batch_size: int = Field(8, s='-B')
    num_workers: int = Field(4, s='-N')
    epoch: int = Field(10, s='-E')
    overwrite:bool = Field(False, s='-O')

class BaseMLCLI(BaseCLI):
    class CommonArgs(BaseMLArgs):
        pass

    def _pre_common(self, a:BaseMLArgs):
        fix_global_seed(self.a.seed)
        super()._pre_common(a)
