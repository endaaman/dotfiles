import os
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from timm.scheduler import CosineLRScheduler
from pydantic import Field
from matplotlib import pyplot as plt
from tqdm import tqdm
from umap import UMAP

from endaaman.ml import BaseTrainer, BaseTrainerConfig, Checkpoint
from endaaman.ml.cli import BaseMLCLI
from endaaman.ml.metrics import BaseMetrics, AccuracyByChannel

J = os.path.join

class CNNModel(nn.Module):
    def __init__(self, input_size=1, output_size=10):
        super().__init__()
        self.output_size = output_size
        self.convs = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, 3, 1, 1),
            nn.ReLU(inplace=True),
        )
        self.pool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Linear(16 * 16, output_size)

    def forward(self, x, activate=False, with_features=False):
        x = self.convs(x)
        x = self.pool(x)
        features = torch.flatten(x, 1)
        x = self.fc(features)
        if activate:
            if self.output_size == 1:
                x = F.sigmoid(x)
            else:
                x = F.log_softmax(x, dim=1)
        if with_features:
            return x, features
        return x


# class TimmModel(nn.Module):
#     def __init__(self, model_name, input_size=1, num_classes=10):
#         super().__init__()
#         self.model_name = model_name
#         self.num_classes = num_classes

#         self.base = timm.create_model(
#                 model_name=model_name,
#                 pretrained=False,
#                 num_classes=self.num_classes,
#                 in_chans=input_size)

#     def forward(self, x, activate=False, with_features=False):
#         x = self.base.forward_features(x)
#         features = self.base.global_pool(x)
#         y = self.base.classifier(features)
#         if activate:
#             y = torch.softmax(y, dim=-1)

#         if with_features:
#             return y, features
#         return y


class TrainerConfig(BaseTrainerConfig):
    # model_name:str = 'resnet18'
    pass



class Trainer(BaseTrainer):
    def metrics_acc(self, preds, gts):
        correct = torch.sum(torch.argmax(preds, dim=-1) == gts)
        return correct / len(preds)

    def _visualize_confusion(self, ax, label, preds, gts):
        preds = torch.argmax(preds, dim=-1)
        cm = confusion_matrix(gts.numpy(), preds.numpy())
        sns.heatmap(cm, annot=True, ax=ax, fmt='g')
        ax.set_title(label)
        ax.set_xlabel('Predict', fontsize=13)
        ax.set_ylabel('GT', fontsize=13)

    def visualize_train_confusion(self, ax, train_preds, train_gts, val_preds, val_gts):
        self._visualize_confusion(ax, 'train', train_preds, train_gts)

    def visualize_val_confusion(self, ax, train_preds, train_gts, val_preds, val_gts):
        self._visualize_confusion(ax, 'val', val_preds, val_gts)

    def prepare(self):
        model = CNNModel(input_size=1, output_size=10)
        self.criterion = nn.CrossEntropyLoss()
        return model

    def eval(self, inputs, gts):
        preds = self.model(inputs.to(self.device))
        loss = self.criterion(preds, gts.to(self.device))
        return loss, preds.detach().cpu()

class CLI(BaseMLCLI):
    class TrainArgs(BaseMLCLI.CommonArgs):
        lr:float = 0.001
        name: str = 'mnist'
        epoch: int = 20
        num_workers: int = 4
        batch_size: int = 32
        overwrite:bool = Field(False, cli=('--overwrite', '-O', ))

    def run_train(self, a:TrainArgs):
        dss = [datasets.MNIST(
            './datasets/MNIST',
            train = t,
            download = True,
            transform = transforms.Compose([
                transforms.ToTensor(),
            ])
        ) for t in [True, False] ]

        # dss[0].data = dss[0].data[:10000, :, :]
        # dss[1].data = dss[1].data[:1000, :, :]

        config = TrainerConfig(
            lr=a.lr,
            batch_size=a.batch_size,
            num_workers=a.num_workers,
        )

        t = Trainer(
            config=config,
            out_dir=f'out/models/{a.name}',
            train_dataset=dss[0],
            val_dataset=dss[1],
            overwrite=a.overwrite,
            fig_size=(20, 8),
            fig_col_count=2,
        )
        t.start(a.epoch)


    class ClusterArgs(BaseMLCLI.CommonArgs):
        model_dir: str = Field(..., s='-d')
        show: bool = False

    def run_cluster(self, a:TrainArgs):
        checkpoint = Checkpoint.from_file(J(a.model_dir, 'checkpoint_last.pt'))
        config = TrainerConfig.from_file(J(a.model_dir, 'config.json'))
        model = CNNModel()
        model.load_state_dict(checkpoint.model_state)
        model = model.eval()

        ds = datasets.MNIST(
            './datasets/MNIST',
            train = False,
            download = True,
            transform = transforms.Compose([
                transforms.ToTensor(),
            ])
        )

        pairs = []
        featuress = []

        for i in tqdm(range(1000)):
            x, gt = ds.__getitem__(i)
            x = x[None, ...]
            with torch.set_grad_enabled(False):
                pred, features = model(x, activate=True, with_features=True)
            pred = torch.argmax(pred)
            pairs.append((gt, pred.item()))
            featuress.append(features[0].detach().numpy())

        pairs = np.array(pairs)
        gts = pairs[:, 0]
        ps = pairs[:, 1]
        featuress = np.stack(featuress)

        print(featuress.shape)

        U_gt, C_gt = np.unique(pairs[:, 0], return_counts=True)
        U_pred, C_pred = np.unique(pairs[:, 1], return_counts=True)

        print(np.asarray((U_gt, C_gt)).T)
        print(np.asarray((U_pred, C_pred)).T)

        embedding = UMAP().fit_transform(featuress)
        embedding_x = embedding[:, 0]
        embedding_y = embedding[:, 1]
        print('done cluster')

        fig, ax = plt.subplots(1, 1, figsize=(14, 10))
        # for p, gt, x, y in zip(ps, gts, embedding_x, embedding_y):
        #     plt.scatter(x, y, s=24, label=gt+1)

        for gt in np.unique(gts):
            needle = gts == gt
            plt.scatter(embedding_x[needle], embedding_y[needle], s=5, label=gt)

        plt.legend()
        plt.show()

if __name__ == '__main__':
    cli = CLI()
    cli.run()
