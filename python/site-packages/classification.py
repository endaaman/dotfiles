import os

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import torch
from torch import nn
import torch.nn.functional as F
from torch import optim
from torch.utils.data import Dataset, DataLoader
import cv2

from torchvision import datasets, transforms
from timm.scheduler import CosineLRScheduler
from pydantic import Field
from matplotlib import pyplot as plt
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from tqdm import tqdm
import timm
from umap import UMAP
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2


from endaaman.ml import BaseTrainer, BaseTrainerConfig, Checkpoint
from endaaman.ml.cli import BaseMLCLI
from endaaman.ml.metrics import BaseMetrics, AccuracyByChannel

J = os.path.join


def get_pool(m):
    if hasattr(m, 'global_pool'):
        return m.global_pool
    return m.head.global_pool

class CNNModel(nn.Module):
    def __init__(self, input_size=1, output_size=10):
        super().__init__()
        self.output_size = output_size
        self.convs = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, 1, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, 3, 1, 1),
            nn.ReLU(inplace=True),
        )
        self.pool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Linear(16 * 16, output_size)

    def forward(self, x, activate=False, with_features=False):
        x = self.convs(x)
        x = self.pool(x)
        features = torch.flatten(x, 1)
        x = self.fc(features)
        if activate:
            if self.output_size == 1:
                x = F.sigmoid(x)
            else:
                x = F.log_softmax(x, dim=1)
        if with_features:
            return x, features
        return x


def create_dataset(mode, is_train, **kwargs):
    if mode == 'number':
        ds = datasets.MNIST(
            './datasets/MNIST',
            train = is_train,
            download = True,
            **kwargs,
        )
        data = ds.data.numpy()[:, None, :, :]
        labels = ds.targets
        albu = A.Compose([
            A.ShiftScaleRotate(
                shift_limit=(-0.1, 0.1), scale_limit=(-0.1, 0.1),
                rotate_limit=(-10, 10), p=1.0
            ),
            A.ElasticTransform(p=1.0, alpha=1, sigma=3, alpha_affine=3),
        ])
        aug_train = lambda x: albu(image=x.squeeze())['image']
        aug_test = lambda x: torch.from_numpy(x/255).to(torch.float32).permute(2, 0, 1)
        return data, labels, aug_train, aug_test
    if mode == 'fashion':
        ds = datasets.FashionMNIST(
            './datasets/FashionMNIST/',
            train = is_train,
            download = True,
            **kwargs,
        )
        data = ds.data.numpy()[:, None, :, :]
        labels = ds.targets
        albu = A.Compose([
            A.ShiftScaleRotate(
                shift_limit=(-0.3, 0.3), scale_limit=(-0.1, 0.1),
                rotate_limit=(-10, 10), p=1.0
            ),
            A.OneOf([
                A.ElasticTransform(p=1.0, alpha=3, sigma=3, alpha_affine=1),
                A.Blur(p=1.0),
            ], p=1.0),
        ])
        aug_train = lambda x: albu(image=x.squeeze())['image']
        aug_test = lambda x: torch.from_numpy(x/255).to(torch.float32).permute(2, 0, 1)
        return data, labels, aug_train, aug_test
    if mode == 'cifar10':
        ds = datasets.CIFAR10(
            './datasets/CIFAR10/',
            train = is_train,
            download = True,
            **kwargs,
        )
        data = ds.data
        labels = torch.tensor(ds.targets)
        norm_args = dict(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))
        albu = A.Compose([
            # A.Rotate(limit=10),
            A.RandomResizedCrop(
                height=32, width=32,
                scale=(0.8, 1.25),
            ),
            # A.ShiftScaleRotate(
            #     shift_limit=(-0.2, 0.2), scale_limit=(-0.2, 0.2),
            #     rotate_limit=(-10, 10), p=0.8
            # ),
            # A.ElasticTransform(p=1.0, alpha=1, sigma=1, alpha_affine=1),
            A.HorizontalFlip(p=0.5),
            # A.OneOf([
            #     A.HueSaturationValue(p=1.0,),
            #     A.RGBShift(p=1.0),
            # ], p=1.0),

            # A.OneOf([
            #     A.ElasticTransform(p=1.0, alpha=3, sigma=3, alpha_affine=1),
            #     A.Blur(p=1.0),
            #     A.OpticalDistortion(p=1.0),
            # ], p=1.0),
            A.Normalize(**norm_args),
            ToTensorV2(),
        ])
        aug_train = lambda x: albu(image=x.squeeze())['image']
        aug_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(**norm_args),
        ])
        return data, labels, aug_train, aug_test
    raise RuntimeError('Invalid dataset', dataset)


class ClsDatgaset(Dataset):
    def __init__(self, dataset:str, is_train:bool, aug:bool):
        self.dataset = dataset
        self.is_train = is_train
        self.aug = aug
        self.data, self.labels, self.aug_train, self.aug_test = create_dataset(dataset, is_train)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, i):
        x = self.data[i]
        label = self.labels[i]
        if self.is_train and self.aug:
            x = self.aug_train(x)
        else:
            x = self.aug_test(x)
        return x, label

class TrainerConfig(BaseTrainerConfig):
    lr:float = 0.01
    model_name: str = Field('resnet18', s='-m', l='--model')
    dataset:str = Field('number', choices=['number', 'fashion', 'cifar10'], s='-d')
    batch_size: int = Field(128, s='-B')
    aug:bool = False



CHANS_MAP = {
    'number': (1, 10),
    'fashion': (1, 10),
    'cifar10': (3, 10),
}


class Trainer(BaseTrainer):
    def metrics_acc(self, preds, gts, batch):
        correct = torch.sum(torch.argmax(preds, dim=-1) == gts)
        return correct / len(preds)

    def _visualize_confusion(self, ax, label, preds, gts):
        preds = torch.argmax(preds, dim=-1)
        cm = confusion_matrix(gts.numpy(), preds.numpy())
        sns.heatmap(cm, annot=True, ax=ax, fmt='g')
        ax.set_title(label)
        ax.set_xlabel('Predict', fontsize=13)
        ax.set_ylabel('GT', fontsize=13)

    def visualize_train_confusion(self, ax, train_preds, train_gts, val_preds, val_gts):
        self._visualize_confusion(ax, 'train', train_preds, train_gts)

    def visualize_val_confusion(self, ax, train_preds, train_gts, val_preds, val_gts):
        self._visualize_confusion(ax, 'val', val_preds, val_gts)

    def create_optimizer(self):
        return optim.RAdam(self.model.parameters(), lr=self.config.lr)

    def create_scheduler(self):
        return optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=10)
        # return optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=10, eta_min=self.config.lr/10)

    def prepare(self):
        if self.config.model_name == 'custom':
            model = CNNModel(input_size=1, output_size=10)
        else:
            chans = CHANS_MAP[self.config.dataset]
            model = timm.create_model(
                model_name=self.config.model_name,
                pretrained=False,
                in_chans=chans[0],
                num_classes=chans[1],
            )
        self.criterion = nn.CrossEntropyLoss()
        return model

    def eval(self, inputs, gts):
        preds = self.model(inputs.to(self.device))
        loss = self.criterion(preds, gts.to(self.device))
        return loss, preds.detach().cpu()

class CLI(BaseMLCLI):
    class TrainArgs(BaseMLCLI.CommonArgs, TrainerConfig):
        epoch: int = Field(50, s='-E')
        num_workers: int = 4
        name: str = Field('{}', s='-n')
        overwrite:bool = Field(False, s='-O')

    def run_train(self, a:TrainArgs):
        dss = [ClsDatgaset(
            dataset=a.dataset,
            is_train=t,
            aug=a.aug
        ) for t in [True, False] ]

        config = TrainerConfig(**a.dict())

        name = a.name.format(a.model_name)
        t = Trainer(
            config=config,
            out_dir=f'out/classification/{a.dataset}/{name}',
            train_dataset=dss[0],
            val_dataset=dss[1],
            overwrite=a.overwrite,
            fig_size=(20, 8),
            fig_col_count=2,
        )
        t.start(a.epoch)


    class ClusterArgs(BaseMLCLI.CommonArgs):
        model_dir: str = Field(..., s='-d')
        show: bool = False
        limit: int= 1000

    def run_cluster(self, a:TrainArgs):
        checkpoint = Checkpoint.from_file(J(a.model_dir, 'checkpoint_last.pt'))
        config = TrainerConfig.from_file(J(a.model_dir, 'config.json'))
        chans = CHANS_MAP[config.dataset]
        model = timm.create_model(
            model_name=config.model_name,
            pretrained=False,
            in_chans=chans[0],
            num_classes=chans[1],
        )
        model.load_state_dict(checkpoint.model_state)
        model = model.eval()
        pool = get_pool(model)

        ds = ClsDatgaset(dataset=config.dataset, is_train=False, aug=False)

        images = ds.data[:a.limit]

        pairs = []
        featuress = []

        for i in tqdm(range(a.limit)):
            x, gt = ds.__getitem__(i)
            x = x[None, ...]
            with torch.set_grad_enabled(False):
                features = model.forward_features(x)
                pred = model.forward_head(features)
                features = pool(features)
            pred = torch.argmax(pred)
            pairs.append((gt, pred.item()))
            featuress.append(features[0].detach().numpy())

        pairs = np.array(pairs)
        gts = pairs[:, 0]
        preds = pairs[:, 1]
        print(gts, preds)
        corrects = gts == preds
        print(corrects)
        featuress = np.stack(featuress)

        print(featuress.shape)

        U_gt, C_gt = np.unique(pairs[:, 0], return_counts=True)
        U_pred, C_pred = np.unique(pairs[:, 1], return_counts=True)

        print(np.asarray((U_gt, C_gt)).T)
        print(np.asarray((U_pred, C_pred)).T)

        embedding = UMAP().fit_transform(featuress)
        embedding_x = embedding[:, 0]
        embedding_y = embedding[:, 1]
        print('done cluster')

        fig, ax = plt.subplots(1, 1, figsize=(14, 10))
        # for p, gt, x, y in zip(ps, gts, embedding_x, embedding_y):
        #     plt.scatter(x, y, s=24, label=gt+1)

        scatters = []
        imagess = []
        for gt in np.unique(gts):
            needle = gts == gt
            xx, yy, cc, ii = embedding_x[needle], embedding_y[needle], corrects[needle], images[needle]
            for t in [True, False]:
                idx = corrects[needle] == t
                scatters.append(plt.scatter(xx[idx], yy[idx], marker='o'if t else 'x', s=5, label=gt))
                imagess.append([cv2.resize(i.squeeze(), (224, 224)) for i in ii[idx]])

        self.hover_images_on_scatters(scatters, imagess, ax=ax)

        plt.legend()
        plt.show()

    def hover_images_on_scatters(self, scatters, imagess, ax=None, ):
        if ax is None:
            ax = plt.gca()
        fig = ax.figure
        imagebox = OffsetImage(imagess[0][0], zoom=.5)
        imagebox.image.axes = ax
        annot = AnnotationBbox(
                imagebox,
                xy=(0, 0),
                # xybox=(256, 256),
                # xycoords='data',
                boxcoords='offset points',
                # boxcoords=('axes fraction', 'data'),
                pad=0.1,
                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=-0.3'))
        annot.set_visible(False)
        ax.add_artist(annot)

        def hover(event):
            vis = annot.get_visible()
            if event.inaxes != ax:
                return
            for n, (sc, ii) in enumerate(zip(scatters, imagess)):
                cont, index = sc.contains(event)
                if cont:
                    i = index['ind'][0]
                    pos = sc.get_offsets()[i]
                    annot.xy = pos
                    annot.xybox = pos + np.array([150, 30])
                    image = ii[i]
                    # text = unique_code[n]
                    # annot.set_text(text)
                    # annot.get_bbox_patch().set_facecolor(cmap(int(text)/10))
                    imagebox.set_data(image)
                    annot.set_visible(True)
                    fig.canvas.draw_idle()
                    return

            if vis:
                annot.set_visible(False)
                fig.canvas.draw_idle()
                return

        fig.canvas.mpl_connect('motion_notify_event', hover)



if __name__ == '__main__':
    cli = CLI()
    cli.run()
