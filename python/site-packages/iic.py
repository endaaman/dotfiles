import os
import sys

import timm
import numpy as np
from sklearn.metrics import confusion_matrix
import torch
from torch import nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import Dataset
from pydantic import Field
from matplotlib import pyplot as plt
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from tqdm import tqdm
from umap import UMAP
import albumentations as A

from endaaman.ml import BaseTrainer, BaseTrainerConfig, tensor_to_pil, pil_to_tensor, Checkpoint
from endaaman.ml.cli import BaseMLCLI, BaseDLArgs
from endaaman.ml.metrics import BaseMetrics, AccuracyByChannel


J = os.path.join


def get_aug():
    return A.Compose([
        A.ShiftScaleRotate(shift_limit=(-0.1, 0.1), scale_limit=(-0.1, 0.1), rotate_limit=(-10, 10), p=1.0),
        A.ElasticTransform(p=1, alpha=1, sigma=3, alpha_affine=3),
    ])


class PairDatgaset(Dataset):
    def __init__(self, is_train:bool, paired:bool, dual:bool):
        self.ds = datasets.MNIST(
            './datasets/MNIST',
            train = is_train,
            download = True,
        )
        self.paired = paired
        self.dual = dual
        self.aug = get_aug()

    def __len__(self):
        return len(self.ds.data)

    def __getitem__(self, i):
        x = self.ds.data[i].numpy()
        y = x
        label = self.ds.targets[i]
        if self.dual:
            x = self.aug(image=x)['image']
        y = self.aug(image=x)['image']
        x = torch.from_numpy(x[None, ...]/255).to(torch.float32)
        y = torch.from_numpy(y[None, ...]/255).to(torch.float32)

        if self.paired:
            same = (torch.rand(1) > 0.5)[0]
            if not same:
                i = torch.randint(len(self.ds.data), (1, ))[0]
                y = self.ds.data[i].numpy()
                y = self.aug(image=y)['image']
                y = torch.from_numpy(y[None, ...]/255).to(torch.float32)
            return x, y, same

        return x, y, label


class IICModel(nn.Module):
    def __init__(self, model_name, input_size=1, num_cluster=10, num_over_cluster=100):
        super().__init__()
        self.num_cluster = num_cluster
        self.num_over_cluster = num_over_cluster
        self.model_name = model_name

        self.base = timm.create_model(
            model_name=model_name,
            pretrained=False,
            num_classes=10,
            in_chans=input_size,
        )

        # num_features = 16 * 16
        self.num_features = self.base.num_features
        self.fc = nn.Linear(self.num_features, num_cluster)
        self.fc_over = nn.Linear(self.num_features, num_over_cluster)

    def forward(self, x, activate=False):
        x = self.base.forward_features(x)
        features = self.base.global_pool(x)
        y = self.fc(features)
        y_over = self.fc_over(features)
        if activate:
            y = torch.softmax(y, dim=-1)
            y_over = torch.softmax(y_over, dim=-1)
            # y = torch.sigmoid(y)
            # y_over = torch.sigmoid(y_over)
        return y, y_over


class IICLoss(nn.Module):
    def __init__(self, alpha):
        super().__init__()
        self.alpha = alpha

    def forward(self, i, j):
        EPS = sys.float_info.epsilon
        bs, k = i.size()
        P = i.unsqueeze(2) * j.unsqueeze(1)
        P = P.sum(dim=0)
        P = (P + P.t()) / 2.0 / P.sum()

        P = torch.clamp(P, min=EPS)

        Pi = P.sum(dim=1).view(k, 1).expand(k, k)
        Pj = P.sum(dim=0).view(1, k).expand(k, k)

        a = self.alpha
        loss = P * (a*Pj.log() + a*Pj.log() - P.log())
        loss = loss.sum()
        return loss

class PairedLoss(nn.Module):
    def __init__(self, alpha):
        super().__init__()
        self.alpha = alpha

    def forward(self, xx, yy, sames):
        EPS = sys.float_info.epsilon
        losses = []
        for x, y, same in zip(xx, yy, sames):
            # pxy = - torch.sum(x * torch.log(y+EPS))
            # pyx = - torch.sum(y * torch.log(x+EPS))
            # loss = pxy + pyx
            if same:
                pxy = - torch.mean(x * torch.log(y+EPS))
                pyx = - torch.mean(y * torch.log(x+EPS))
                loss = (pxy + pyx)/2
            else:
                loss = - torch.mean(torch.log(x) * torch.log(y) / torch.log(x * y))
                loss *= self.alpha
            losses.append(loss)
        return sum(losses) / len(losses)


class CrossEntropyLoss(nn.Module):
    def __init__(self, eps=1e-32, input_logits=True):
        super().__init__()
        self.eps = eps
        self.input_logits = input_logits

    # y: target index
    def forward(self, x, y):
        if self.input_logits:
            x = torch.softmax(x, dim=-1)
        return F.nll_loss(torch.clamp(x, self.eps).log(), y)


class TrainerConfig(BaseTrainerConfig):
    model_name:str
    num_cluster:int = 10
    num_over_cluster:int = 100
    dual: bool = False
    paired: bool = False
    alpha: float= 2.0


class Trainer(BaseTrainer):
    def metrics_precision(self, preds, gts, batch):
        if batch:
            return None
        preds = preds.detach().cpu()
        labels = torch.unique(preds)
        correct = 0
        for label in labels:
            items = gts[preds == label]
            elements, counts = torch.unique(items, return_counts=True)
            dominant = elements[torch.argmax(counts)]
            # print(label, dominant, torch.sum(items == dominant))
            correct += torch.sum(items == dominant)
        return correct/len(preds)
        # return correct / len(preds)

    # def _visualize_confusion(self, ax, label, preds, gts):
    #     # preds = torch.argmax(preds, dim=-1)
    #     # cm = confusion_matrix(gts.numpy(), preds.numpy())
    #     # sns.heatmap(cm, annot=True, ax=ax, fmt='g')
    #     # ax.set_title(label)
    #     x = np.arange(100)
    #     y = x ** 2
    #     ax.plot(x, y)
    #     ax.set_xlabel('Predict', fontsize=13)
    #     ax.set_ylabel('GT', fontsize=13)

    # def visualize_train_acc(self, ax, train_preds, train_gts, val_preds, val_gts):
    #     self._visualize_confusion(ax, 'train', train_preds, train_gts)

    # def visualize_val_acc(self, ax, train_preds, train_gts, val_preds, val_gts):
    #     self._visualize_confusion(ax, 'val', val_preds, val_gts)

    def prepare(self):
        model = IICModel(
            input_size=1,
            model_name=self.config.model_name,
            num_cluster=self.config.num_cluster,
            num_over_cluster=self.config.num_over_cluster
        )
        # self.criterion = CrossEntropyLoss(input_logits=False)
        if self.config.paired:
            Loss = PairedLoss
        else:
            Loss = IICLoss
        self.criterion = Loss(alpha=self.config.alpha)
        return model

    def select_inputs_and_gts(self, params):
        return params[0], params[2]

    def eval(self, *args):
        if self.config.paired:
            xx, yy, sames = args
        else:
            xx, yy, __labels = args
        x_pred, x_pred_over = self.model(xx.to(self.device), activate=True)
        y_pred, y_pred_over = self.model(yy.to(self.device), activate=True)

        if self.config.paired:
            loss = self.criterion(x_pred, y_pred, sames)
            loss_over = self.criterion(x_pred_over, y_pred_over, sames)
        else:
            loss = self.criterion(x_pred, y_pred)
            loss_over = self.criterion(x_pred_over, y_pred_over)
        total_loss = (loss + loss_over) / 2
        # total_loss = loss
        return total_loss, torch.argmax(x_pred, dim=1)


class CLI(BaseMLCLI):
    class CommonArgs(BaseMLCLI.CommonArgs):
        pass

    def run_model(self, a):
        x = torch.randn(4, 1, 28, 28)
        model = IICModel(x)
        y, y0 = model(x)
        print(y.shape, y0.shape)

    class TrainArgs(CommonArgs):
        lr:float = 0.001
        epoch: int = Field(100, s='-E')
        model_name: str = Field('resnet18', s='-m', l='--model')
        num_workers: int = 4
        batch_size: int = Field(512, s='-B')
        paired: bool = False
        dual: bool = False
        suffix: str = Field('', s='-S')
        overwrite:bool = Field(False, s='-O')
        alpha:float = 2.0

    def run_train(self, a:TrainArgs):
        dss = [
            PairDatgaset(is_train='train'==t, paired=a.paired, dual=a.dual)
            for t in [True, False]
        ]

        config = TrainerConfig(
            lr=a.lr,
            model_name=a.model_name,
            batch_size=a.batch_size,
            num_workers=a.num_workers,
            paired=a.paired,
            dual=a.dual,
            alpha=a.alpha,
        )


        name = 'pairs' if a.paired else 'iic'
        d = self.with_suffix(name, a.suffix)

        t = Trainer(
            # loss_fmt='{:.10f}',
            config=config,
            out_dir=f'out/{name}/{d}',
            train_dataset=dss[0],
            val_dataset=dss[1],
            overwrite=a.overwrite,
            fig_size=(20, 8),
            fig_col_count=2,
        )
        t.start(a.epoch)

    class ClusterArgs(BaseDLArgs):
        model_dir: str = Field(..., s='-d')
        show: bool = False
        limit: int = 1000

    def run_cluster(self, a):
        checkpoint = Checkpoint.from_file(J(a.model_dir, 'checkpoint_last.pt'))
        config = TrainerConfig.from_file(J(a.model_dir, 'config.json'))
        model = IICModel(model_name=config.model_name)
        model.load_state_dict(checkpoint.model_state)
        model = model.eval()

        ds = datasets.MNIST(
            './datasets/MNIST',
            train = False,
            download = True,
            transform = transforms.Compose([
                transforms.ToTensor(),
            ])
        )

        pairs = []
        preds = []

        images = ds.data[:a.limit].numpy()

        for i in tqdm(range(a.limit)):
            x, gt = ds.__getitem__(i)
            x = x[None, ...]
            with torch.no_grad():
                pred, y_o = model(x, activate=True)
            # pred = y_o
            p = torch.argmax(pred)
            pairs.append((gt, p.item()))
            preds.append(pred.detach().numpy()[0])
            # plt.imshow(x.squeeze())
            # plt.show()

        pairs = np.array(pairs)
        gts = pairs[:, 0]
        ps = pairs[:, 1]
        preds = np.stack(preds)

        np.save(self.with_wrote(J(a.model_dir, 'pairs')), pairs)

        print(preds.shape)

        U_gt, C_gt = np.unique(pairs[:, 0], return_counts=True)
        U_pred, C_pred = np.unique(pairs[:, 1], return_counts=True)

        print(np.asarray((U_gt, C_gt)).T)
        print(np.asarray((U_pred, C_pred)).T)

        embedding = UMAP().fit_transform(preds)
        embedding_x = embedding[:, 0]
        embedding_y = embedding[:, 1]
        print('done cluster')

        fig, ax = plt.subplots(1, 1, figsize=(14, 10))
        scs = []
        iis = []
        print(gts.shape)
        print(images.shape)
        for gt in np.unique(gts):
            needle = gts == gt
            print(needle)
            scs.append(plt.scatter(embedding_x[needle], embedding_y[needle], s=5, label=gt))
            iis.append(images[needle])


        imagebox = OffsetImage(images[0], zoom=.5)
        imagebox.image.axes = ax
        annot = AnnotationBbox(
                imagebox,
                xy=(0, 0),
                # xybox=(256, 256),
                # xycoords='data',
                boxcoords='offset points',
                # boxcoords=('axes fraction', 'data'),
                pad=0.1,
                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=-0.3'))
        annot.set_visible(False)
        ax.add_artist(annot)

        def hover(event):
            vis = annot.get_visible()
            if event.inaxes != ax:
                return
            for n, (sc, ii) in enumerate(zip(scs, iis)):
                cont, index = sc.contains(event)
                if cont:
                    i = index['ind'][0]
                    pos = sc.get_offsets()[i]
                    annot.xy = pos
                    annot.xybox = pos + np.array([150, 30])
                    image = ii[i]
                    # text = unique_code[n]
                    # annot.set_text(text)
                    # annot.get_bbox_patch().set_facecolor(cmap(int(text)/10))
                    imagebox.set_data(image)
                    annot.set_visible(True)
                    fig.canvas.draw_idle()
                    return

            if vis:
                annot.set_visible(False)
                fig.canvas.draw_idle()
                return

        fig.canvas.mpl_connect('motion_notify_event', hover)


        plt.legend()
        plt.show()


    class LossArgs(CommonArgs):
        pass

    def loss(self, x, y, same):
        if same:
            return - torch.sum(x * torch.log(y) + y * torch.log(x))
        # return torch.sum(x * y)
        return torch.sum(x * torch.log(1-y) + y * torch.log(1-x))

    def run_loss(self, a):
        x = torch.tensor([0.1, 0.9])
        y = torch.tensor([0.3, 0.7])
        z = torch.tensor([0.9, 0.1])

        print('same', x, y)
        print(self.loss(x, y, True))

        print('diff', x, y)
        print(self.loss(x, y, False))

        print('same', x, z)
        print(self.loss(x, z, True))

        print('diff', x, z)
        print(self.loss(x, z, False))

    def run_aug(self, a):
        ds = datasets.MNIST(
            './datasets/MNIST',
            train = False,
            download = True,
        )
        xx = ds.data[:20].numpy()

        aug = get_aug()

        xx2 = []
        xx3 = []
        for x in xx:
            xx2.append(aug(image=x)['image'])
            xx3.append(aug(image=x)['image'])
        xx = np.concatenate(xx, axis=1)
        xx2 = np.concatenate(xx2, axis=1)
        xx3 = np.concatenate(xx3, axis=1)
        print(xx.shape)
        print(xx2.shape)
        plt.imshow(np.concatenate([xx, xx2, xx3], axis=0))
        plt.show()



print('enter done')

if __name__ == '__main__':
    cli = CLI()
    cli.run()
