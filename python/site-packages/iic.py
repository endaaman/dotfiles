import os
import sys

import timm
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import torch
import torchvision as tv
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torch.utils.data import Dataset
from timm.scheduler import CosineLRScheduler
from pydantic import Field
from matplotlib import pyplot as plt
from tqdm import tqdm
import umap

from endaaman.ml import BaseTrainer, BaseTrainerConfig, tensor_to_pil, pil_to_tensor
from endaaman.ml.cli2 import BaseMLCLI, BaseDLArgs
from endaaman.ml.metrics import BaseMetrics, AccuracyByChannel

J = os.path.join


class IICModel(nn.Module):
    def __init__(self, input_size=1, num_cluster=10, num_over_cluster=100):
        super().__init__()
        self.num_cluster = num_cluster
        self.num_over_cluster = num_over_cluster

        self.base = timm.create_model(
                'resnet18',
                pretrained=False,
                num_classes=10,
                in_chans=input_size)

        # num_features = 16 * 16
        num_features = 512
        self.fc = nn.Linear(num_features, num_cluster)
        self.fc_over = nn.Linear(num_features, num_over_cluster)

    def forward(self, x, activate=False):
        x = self.base.forward_features(x)
        features = self.base.global_pool(x)
        y = self.fc(features)
        y_over = self.fc_over(features)
        if activate:
            y = torch.softmax(y, dim=-1)
            y_over = torch.softmax(y_over, dim=-1)
            # y = torch.sigmoid(y)
            # y_over = torch.sigmoid(y_over)
        return y, y_over


def perturb_imagedata(x):
    y = x.clone()
    batch_size = x.size(0)

    # ランダムなアフィン変換を実施
    trans = tv.transforms.RandomAffine(15, (0.2, 0.2,), (0.2, 0.75,))
    for i in range(batch_size):
        y[i, 0] = pil_to_tensor(trans(tensor_to_pil(y[i, 0])))

    # ノイズを加える
    noise = torch.randn(batch_size, 1, x.size(2), x.size(3))
    div = torch.randint(20, 30, (batch_size,),
                        dtype=torch.float32).view(batch_size, 1, 1, 1)
    y += noise / div

    return y

def compute_joint(x_out, x_tf_out):
    # x_out、x_tf_outは torch.Size([512, 10])。
    # この二つをかけ算して同時分布を求める、torch.Size([2048, 10, 10])にする。
    # torch.Size([512, 10, 1]) * torch.Size([512, 1, 10])
    p_i_j = x_out.unsqueeze(2) * x_tf_out.unsqueeze(1)
    # p_i_j は　torch.Size([512, 10, 10])

    # 全ミニバッチを足し算する ⇒ torch.Size([10, 10])
    p_i_j = p_i_j.sum(dim=0)

    # 転置行列と足し算して割り算（対称化） ⇒ torch.Size([10, 10])
    p_i_j = (p_i_j + p_i_j.t()) / 2.

    # 規格化 ⇒ torch.Size([10, 10])
    p_i_j = p_i_j / p_i_j.sum()

    # 結局、p_i_jは通常画像の判定出力10種類と、変換画像の判定10種類の100パターンに対して、
    # 全ミニバッチが100パターンのどれだったのかの確率分布表を示す
    return p_i_j


def iic_loss(x_out, x_tf_out, EPS=sys.float_info.epsilon):
    # torch.Size([512, 10])、後ろの10は分類数なので、overclusteringのときは100
    bs, k = x_out.size()
    p_i_j = compute_joint(x_out, x_tf_out)  # torch.Size([10, 10])

    # 同時確率の分布表から、変換画像の10パターンをsumをして周辺化し、元画像だけの周辺確率の分布表を作る
    p_i = p_i_j.sum(dim=1).view(k, 1).expand(k, k)
    # 同時確率の分布表から、元画像の10パターンをsumをして周辺化し、変換画像だけの周辺確率の分布表を作る
    p_j = p_i_j.sum(dim=0).view(1, k).expand(k, k)

    # 0に近い値をlogに入れると発散するので、避ける
    #p_i_j[(p_i_j < EPS).data] = EPS
    #p_j[(p_j < EPS).data] = EPS
    #p_i[(p_i < EPS).data] = EPS
    # 参考GitHubの実装（↑）は、PyTorchのバージョン1.3以上だとエラーになる
    # https://discuss.pytorch.org/t/pytorch-1-3-showing-an-error-perhaps-for-loss-computed-from-paired-outputs/68790/3

    # 0に近い値をlogに入れると発散するので、避ける
    p_i_j = torch.where(p_i_j < EPS, torch.tensor(
        [EPS], device=p_i_j.device), p_i_j)
    p_j = torch.where(p_j < EPS, torch.tensor([EPS], device=p_j.device), p_j)
    p_i = torch.where(p_i < EPS, torch.tensor([EPS], device=p_i.device), p_i)

    # 元画像、変換画像の同時確率と周辺確率から、相互情報量を計算
    # ただし、マイナスをかけて最小化問題にする
    """
    相互情報量を最大化したい
    ⇒結局、x_out, x_tf_outが持ちあう情報量が多くなって欲しい
    ⇒要は、x_out, x_tf_outが一緒になって欲しい

    p_i_jはx_out, x_tf_outの同時確率分布で、ミニバッチが極力、10×10のいろんなパターン、満遍なく一様が嬉しい

    前半の項、torch.log(p_i_j)はp_ijがどれも1に近いと大きな値（0に近い）になる。
    どれかが1であと0でバラついていないと、log0で小さな値（負の大きな値）になる
    つまり前半の項は、

    後半の項は、元画像、もしくは変換画像について、それぞれ周辺化して10通りのどれになるかを計算した項。
    周辺化した10×10のパターンを引き算して、前半の項が小さくなるのであれば、
    x_outとx_tf_outはあまり情報を共有していなかったことになる。
    """
    # https://qiita.com/Amanokawa/items/0aa24bc396dd88fb7d2a
    # を参考に、重みalphaを追加
    # 同時確率分布表のばらつきによる罰則を小さく ＝ 同時確率の分布がバラつきやすくする
    alpha = 2.0  # 論文や通常の相互情報量の計算はalphaは1です

    loss = -1*(p_i_j * (torch.log(p_i_j) - alpha * torch.log(p_j) - alpha*torch.log(p_i))).sum()

    return loss


def pc_loss(xx, yy, sames, EPS=sys.float_info.epsilon):
    losses = []
    for x, y, same in zip(xx, yy, sames):
        # pxy = - torch.sum(x * torch.log(y+EPS))
        # pyx = - torch.sum(y * torch.log(x+EPS))
        # loss = pxy + pyx

        if same:
            pxy = - torch.sum(x * torch.log(y+EPS))
            pyx = - torch.sum(y * torch.log(x+EPS))
            loss = pxy + pyx
        else:
            pxy = - torch.sum(x * torch.log(1-y+EPS))
            pyx = - torch.sum(y * torch.log(1-x+EPS))
            loss = pxy + pyx
        if loss < 0:
            print(xx)
            print(yy)
            print(loss)
            exit(0)
        losses.append(loss)
    return sum(losses) / len(losses)


class CrossEntropyLoss(nn.Module):
    def __init__(self, eps=1e-32, input_logits=True):
        super().__init__()
        self.eps = eps
        self.input_logits = input_logits

    # y: target index
    def forward(self, x, y):
        if self.input_logits:
            x = torch.softmax(x, dim=-1)
        return F.nll_loss(torch.clamp(x, self.eps).log(), y)


def aug(x):
    y = x.clone()
    trans = tv.transforms.RandomAffine(15, (0.2, 0.2,), (0.2, 0.75,))
    y = pil_to_tensor(trans(tensor_to_pil(y)))

    noise = torch.randn(1, x.size(-2), x.size(-1))
    div = torch.randint(20, 30, (1,), dtype=torch.float32).view(1, 1, 1)
    y += noise / div
    return y

class PairDatgaset(Dataset):
    def __init__(self, is_train):
        self.ds = datasets.MNIST(
            './datasets/MNIST',
            train = is_train,
            download = True,
            transform = transforms.Compose([
                transforms.ToTensor(),
            ])
        )

    def __len__(self):
        return len(self.ds.data)

    def __getitem__(self, i):
        same = np.random.rand() > 0.5
        x = self.ds.data[i]
        if same:
            y = aug(x)
        else:
            y = self.ds.data[np.random.randint(len(self.ds.data))]
            y = y[None, :, :]/255
        x = x[None, :, :]/255
        return x, y, same


class TrainerConfig(BaseTrainerConfig):
    num_cluster:int = 10
    num_over_cluster:int = 100


class Trainer(BaseTrainer):
    # def metrics_acc(self, preds, gts):
    #     correct = torch.sum(torch.argmax(preds, dim=-1) == gts)
    #     return correct / len(preds)

    # def _visualize_confusion(self, ax, label, preds, gts):
    #     preds = torch.argmax(preds, dim=-1)
    #     cm = confusion_matrix(gts.numpy(), preds.numpy())
    #     sns.heatmap(cm, annot=True, ax=ax, fmt='g')
    #     ax.set_title(label)
    #     ax.set_xlabel('Predict', fontsize=13)
    #     ax.set_ylabel('GT', fontsize=13)

    # def visualize_train_confusion(self, ax, train_preds, train_gts, val_preds, val_gts):
    #     self._visualize_confusion(ax, 'train', train_preds, train_gts)

    # def visualize_val_confusion(self, ax, train_preds, train_gts, val_preds, val_gts):
    #     self._visualize_confusion(ax, 'val', val_preds, val_gts)

    def prepare(self):
        model = IICModel(
            input_size=1,
            num_cluster=self.config.num_cluster,
            num_over_cluster=self.config.num_over_cluster
        )
        self.criterion = CrossEntropyLoss(input_logits=False)
        return model

    def eval(self, xx, yy, sames):
        x_pred, x_pred_over = self.model(xx.to(self.device), activate=True)
        y_pred, y_pred_over = self.model(yy.to(self.device), activate=True)

        loss = pc_loss(x_pred, y_pred, sames)
        loss_over = pc_loss(x_pred_over, y_pred_over, sames)
        total_loss = loss + loss_over
        # if total_loss < 0:
        #     exit(0)

        return total_loss, None
        # return 1/torch.log(-total_loss), None
        # return 1/-total_loss, None
        # return torch.exp(total_loss), None

        # loss = self.criterion(data1, gts.to(self.device))
        # return loss


class CLI(BaseMLCLI):
    class CommonArgs(BaseMLCLI.CommonArgs):
        pass

    def run_model(self, a):
        x = torch.randn(4, 1, 28, 28)
        model = IICModel(x)
        y, y0 = model(x)
        print(y.shape, y0.shape)

    class TrainArgs(CommonArgs):
        lr:float = 0.001
        name: str = 'iic'
        epoch: int = 20
        num_workers: int = 4
        batch_size: int = 32
        overwrite:bool = Field(False, s='-O')

    def run_train(self, a:TrainArgs):
        dss = [
            PairDatgaset(is_train='train'==t)
            for t in [True, False]
        ]

        config = TrainerConfig(
            lr=a.lr,
            batch_size=a.batch_size,
            num_workers=a.num_workers,
        )

        t = Trainer(
            loss_fmt='{:.10f}',
            config=config,
            out_dir=f'out/models/{a.name}',
            train_dataset=dss[0],
            val_dataset=dss[1],
            overwrite=a.overwrite,
            fig_size=(20, 8),
            fig_col_count=2,
        )
        t.start(a.epoch)

    class PredArgs(BaseDLArgs):
        model_dir: str = Field(..., s='-d')
        show: bool = False

    def run_pred(self, a):
        checkpoint:Checkpoint = torch.load(J(a.model_dir, 'checkpoint_last.pt'), map_location='cpu')
        config = TrainerConfig.from_file(J(a.model_dir, 'config.json'))
        model = IICModel()
        model.load_state_dict(checkpoint.model_state)
        # model.to(a.device())
        model = model.eval()

        ds = datasets.MNIST(
            './datasets/MNIST',
            train = False,
            download = True,
            transform = transforms.Compose([
                transforms.ToTensor(),
            ])
        )

        pairs = []
        preds = []

        for i in tqdm(range(1000)):
            x, gt = ds.__getitem__(i)
            x = x[None, ...]
            pred, y_o = model(x, activate=True)
            # pred = y_o
            p = torch.argmax(pred)
            pairs.append((gt, p.item()))
            preds.append(pred.detach().numpy()[0])
            # plt.imshow(x.squeeze())
            # plt.show()

        pairs = np.array(pairs)
        gts = pairs[:, 0]
        ps = pairs[:, 1]
        preds = np.stack(preds)

        print(preds.shape)

        U_gt, C_gt = np.unique(pairs[:, 0], return_counts=True)
        U_pred, C_pred = np.unique(pairs[:, 1], return_counts=True)

        print(np.asarray((U_gt, C_gt)).T)
        print(np.asarray((U_pred, C_pred)).T)

        print('done cluster')

        embedding = umap.UMAP().fit_transform(preds)
        embedding_x = embedding[:, 0]
        embedding_y = embedding[:, 1]

        fig, ax = plt.subplots(1, 1, figsize=(14, 10))
        # for p, gt, x, y in zip(ps, gts, embedding_x, embedding_y):
        #     plt.scatter(x, y, s=24, label=gt+1)

        for gt in np.unique(gts):
            needle = gts == gt
            plt.scatter(embedding_x[needle], embedding_y[needle], s=5, label=gt)

        plt.legend()
        plt.show()


    class LossArgs(CommonArgs):
        pass

    def loss(self, x, y, same):
        if same:
            return - torch.sum(x * torch.log(y) + y * torch.log(x))
        # return torch.sum(x * y)
        return torch.sum(x * torch.log(1-y) + y * torch.log(1-x))

    def run_loss(self, a):
        x = torch.tensor([0.1, 0.9])
        y = torch.tensor([0.3, 0.7])
        z = torch.tensor([0.9, 0.1])

        print('same', x, y)
        print(self.loss(x, y, True))

        print('diff', x, y)
        print(self.loss(x, y, False))

        print('same', x, z)
        print(self.loss(x, z, True))

        print('diff', x, z)
        print(self.loss(x, z, False))


if __name__ == '__main__':
    cli = CLI()
    cli.run()
